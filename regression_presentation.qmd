---
title: "DIY Regression"
author: "Saumya Seth, Jasmine Siswandjo"
format: revealjs
incremental: true
editor: visual
---

```{r}
library(tidyverse)
library(alr4)
library(pROC)
library(ROCR)
library(ggeffects)
```

# DIY Regression

Fall 2023

Statistical Consulting Research Seminar

# Logistic and Probit Regression Models

## When are they used?

-   When the outcome is binary - thus, mainly used for classification problems

-   When the outcome is categorical with more than 2 categories, we use the multinomial logistic or probit regression. We will focus on simple logistic and probit regression here.

## Using logistic regression vs linear probability model

-   When covariates are continuous, there are infinite possible values for the outcome if using linear regression. 

-   Logistic is therefore better than linear if we need to bound the outcome to 0 and 1. 


## Logit v/s Probit

-   Since we know that logistic regression is also helpful in predicting binary outcomes, what is the difference between the two?

-   They give almost identical results - they just have different link functions (more on this later)


## Logit v/s Probit

-   If z is the latent variable that we do not observe and p is the probability of Y being 1, then,

    Logit has the link function:

    $$z = log(\frac{p}{1-p})$$

    Probit has the link function:

    $$
    z = \phi^{-1}(p) 
    \text{ where } \phi \text{ is the cumulative}\\
    \text{normal probability density function}
    $$

## Logit v/s Probit

-   The decision to chose one over the other is discipline-dependent, and it is said that logit is better when you have extreme independent variables (where one particular small or large value will overwhelmingly determine if your outcome is 0 or 1 - overriding the effect of most other variables)

-   There is no 'right' answer to this debate



## We used 'optim' to maximize our log likelihood function

-   We were a bit unsure about which method of optimization to use. We decided to use optim as it proved to be user-friendly for us

-   Optim is a function in R which takes in a parameter to optimize and the function whose minimum/maximum needs to be found


## Using the `optim` function

::: {.nonincremental}
1.  Define the predictor variables
2.  Define the outcome variable
3.  Define the log likelihood function - this will help estimate our betas (regression coefficients)
4.  Define an initial guess to update later in order to maximize the log likelihood
5.  Iterate over the guesses using `optim` to find the parameter which maximizes the log likelihood of seeing the data inputted
:::

## Using the `optim` function

-   There are many ways that optim can optimize the parameter value. We used a method called 'BFGS' (Broyden Fletcher Goldfarb Shanno)

-   BFGS works like this:

    -   it begins with an initial guess of the parameter value

    -   this guess updates after every iteration of calculating the gradient of the log likelihood function

    -   the updates stop when further iterations no longer further maximize the log likelihood


## Logistic regression assumptions

-   Response variable should be linearly related to the log odds  

## Logistic regression

```{r}
source("./logistic.R")
```

```{r echo=T}
optim_logistic <- function(beta, X, Y) {
  beta <- as.matrix(beta, nrow = 4)
  pi <- plogis(X %*% beta)
  loglikelihood <- -sum(Y * log(pi) + (1 - Y) * log(1 - pi))
  return(loglikelihood)
}
```

```{=tex}
\begin{align*}
\pi &= X_i\beta\\
\ell(\beta_0, \beta) &= \prod^{n}_{i=1}\pi^{y_i}(1-\pi)^{1-y_i}\\
&=\sum^{n}_{i=1} y_i \cdot log(\pi) + (1-y_i) * log(1-\pi)
\end{align*}
```
## Simulate some data

```{r echo=T}
sim_data <- data.frame(
  x1 = rnorm(100, 2, 1), 
  x2 = rnorm(100, 4, 1), 
  x3 = rnorm(100, 6, 1))

sim_data$y <- rbinom(
  100, 
  size = 1, 
  prob = plogis(-1 + sim_data$x1 + sim_data$x2 - 0.5 * sim_data$x3)
)
```

## Output from GLM {auto-animate="true"}

## Output from GLM {auto-animate="true"}

```{r}
cat("GLM logistic function")
fit_sim_data <- glm(y ~ x1 + x2 + x3, data = sim_data, family = binomial)
t(summary(fit_sim_data)$coef)[1:2, ]
```

## Output from GLM {auto-animate="true"}

```{r}
cat("GLM logistic function")
fit_sim_data <- glm(y ~ x1 + x2 + x3, data = sim_data, family = binomial)
t(summary(fit_sim_data)$coef)[1:2, ]


cat("\nOur logistic function")
logistic_function(fn_formula = "y ~ x1 + x2 + x3", data = sim_data)
```

## Also works for categorical variables

```{r}
Donner$survived <- Donner$y == "survived"
fit_Donner <- glm(survived ~ age + sex + status, data = Donner, family = "binomial")
```

```{r echo=T}
summary(fit_Donner)$coef
t(logistic_function(fn_formula = "survived ~ age + sex + status", data = Donner))
```

## How it works for categorical variables

```{r echo=T}
logistic_function <- function(fn_formula, data) {
  ...
  vars <- all.vars(as.formula(fn_formula))
  y_name <- vars[1]
  x_name <- vars[2:length(vars)]
  n <- nrow(data)
  Y <- matrix(data[, y_name], nrow = n, ncol = 1)
  X <- matrix(cbind(rep(1, n)))

  # take in categorical data
  var_names <- vector("character")
  for (i in x_name) {
    if (suppressWarnings(all(!is.na(as.numeric(as.character(data[, i])))))) {
      X <- cbind(X, as.numeric(as.character(data[, i])))
      var_names <- c(var_names, i)
    } else {
      categories <- sort(unique(data[, i]))
      for (j in categories[2:length(categories)]) {
        new_col_name <- paste0(i, j)
        new_col <- ifelse(data[, i] == j, 1, 0)
        X <- cbind(X, new_col)
        var_names <- c(var_names, new_col_name)
      }
    }
  }
}
```

-   supply the formula `logistic_function(fn_formula = "survived ~ age + sex + status", data = Donner)`

## How it works for categorical variables

```{r echo=T}
logistic_function <- function(fn_formula, data) {
  ...
  vars <- all.vars(as.formula(fn_formula))
  y_name <- vars[1]
  x_name <- vars[2:length(vars)]
  n <- nrow(data)
  Y <- matrix(data[, y_name], nrow = n, ncol = 1)
  X <- matrix(cbind(rep(1, n)))

  # take in categorical data
  var_names <- vector("character")
  for (i in x_name) {
    if (suppressWarnings(all(!is.na(as.numeric(as.character(data[, i])))))) {
      X <- cbind(X, as.numeric(as.character(data[, i])))
      var_names <- c(var_names, i)
    } else {
      categories <- sort(unique(data[, i]))
      for (j in categories[2:length(categories)]) {
        new_col_name <- paste0(i, j)
        new_col <- ifelse(data[, i] == j, 1, 0)
        X <- cbind(X, new_col)
        var_names <- c(var_names, new_col_name)
      }
    }
  }
}
```

::: nonincremental
-   creates dummy column for every non-numeric variable (i.e. 1 if sexMale, 0 if not male)
:::

# Probit Regression


## Our Implementation

```{r, echo=T, eval=F}
probit_regression <- function(data, x1, x2, ..., y) {
  # defining predictor variables
  X <-
      matrix(c(rep(1, n), x_parameters),
             nrow = n,
             ncol = ncol(data))
  # defining outcome variable
  Y <- matrix(y, nrow = n, ncol = 1)
    # defining the log likelihood function
  probit.loglikelihood <- function(beta, X, Y) {
      eta <- X %*% beta
      p <- pnorm(eta)
      loglikelihood <- -sum((1 - Y) * log(1 - p) + Y * log(p))
      return(loglikelihood)
  }
  # defining an initial guess
    initial_guess <- matrix(0, nrow = ncol(data), ncol = 1)
    # optimizing by updating the guesses
    result <-
      optim(
        initial_guess,
        probit.loglikelihood,
        X = X,
        Y = Y,
        method = 'BFGS'
      )
}
```

## We get similar values as R

```{r, eval=T, echo=T}
source('./probit.R')
# our output
our_implementation_probit
# r's output
r_implementation_probit
```

## How does it work?

-   In linear regression, we tried to minimize the sum of least squares

-   In probit regression, we use Maximum Likelihood Estimation to get at the best estimate of our betas (regression coefficients)

## Maximum Likelihood review

-   MLE helps us determine which parameter (beta) our data was most likely to come from

-   It tries to maximize the likelihood of seeing our data given various curves (parameter values)

::: fragment
```{r, echo=F, eval=T, fig.height=3.5}
x0 <- seq(-3, 3, length.out = 20)
x1 <- seq(-3, 3, length.out = 20) + 0.3
y <- pnorm(x0)
plot(x0, y, type = 'l', col = 'green', ylim = c(0, 1.2),
     cex.main = 0.7, ylab = 'y', xlab = 'x')
lines(x1, y, col = 'red')
points(x0, y + 0.05, type = 'p', col = 'blue')
```
:::


## What exactly are we trying to estimate - what is the latent variable?

-   Our latent variable is z (we do not observe it):

::: fragment
$$
z = \beta_0 + \beta_1x_1 + ... + \beta_nx_n + \epsilon
$$
:::

-   We want to somehow link z to the probability of our **ACTUALLY OBSERVED** outcome (Y) being 1. We do this using the link function phi

::: fragment
$$
P(Y = 1) = \phi(z)
$$
:::

## What exactly is the latent variable?

-   Now, to find z, we take the inverse of phi on both sides

::: fragment
$$
\phi^{-1}(P(Y = 1)) = z
$$
:::

-   Phi inverse thus helps us transform a probability to the latent variable z

::: fragment
```{r, echo=F, eval=T, fig.height=3}
z_values <- seq(-3, 3, length.out = 100)
cumulative_probs <- pnorm(z_values)
plot(z_values, cumulative_probs, type = "l", 
     xlab = "Latent Variable (z)", ylab = "Cumulative Probability", 
     main = "Relationship between z and Cumulative Probabilities")
```
:::

## What exactly is the latent variable?

-   We can rewrite our model in the following way:

::: fragment
$$
\phi^{-1}(P(Y = 1)) = \beta_0 + \beta_1x_1 + ... + \beta_nx_n + \epsilon
$$
:::

-   Note that our outcome is a z-score now

-   Thus, when we estimate our betas using Maximum Likelihood, we interpret them as being a **change in the z-score of the outcome that we observe**. We then transform this z score to a probability using the z-table

## Assumptions of Probit Regression

-   The outcome is binary

-   The **z-score of the outcome** and the predictor variables have a linear relationship

-   The errors are normally distributed and are independent of one another

## An example analysis

-   Outcome 'admit' (binary) and predictor 'gpa'

::: fragment
```{r, eval=T, echo=F}
my_data <- read.csv('https://stats.idre.ucla.edu/stat/data/binary.csv')
head(my_data)
```
:::

## An example analysis

```{r, eval=T, echo=F}
ggplot(my_data, aes(x = gpa, y = admit)) +
      geom_point() +
      geom_smooth(method = "loess", se = FALSE) +
      theme_classic()
```

## An example analysis

-   Fitting the model

::: fragment
```{r}
fit_probit_correct <- glm(admit ~ gpa + gre, data = my_data,
                 family = binomial(link = 'probit'))
summary(fit_probit_correct)$coef
```
:::

-   Interpreting the coefficient of gpa: Comparing 2 groups of people, one who got admitted and one who did not, the average gpa of those who did get admitted is around 0.45 z-scores higher than the group which did not

## The correct model: Linearity Assumption

-   Checking if the z-score of the outcome and predictor have a linear relationship

```{r}
my_data$probabilities_correct <- fit_probit_correct$fitted.values
my_data <- my_data %>%
    mutate(probit_correct = qnorm(probabilities_correct))
ggplot(my_data, aes(y = probit_correct, x = gpa))+
    geom_point(size = 0.5, alpha = 0.5) +
    geom_smooth(method = 'loess') +
    theme_bw()
```

:::

## The correct model: AUC

```{r, echo=T, eval=T}
train_indices <- sample(x = nrow(my_data), size = 320, replace = F)
test_indices <- c(1:400)[-train_indices]
train_probit <- my_data[train_indices,]
test_probit <- my_data[test_indices,]
predicted_probit_correct <- predict(fit_probit_correct, test_probit, type='response')
pred_probit_correct  <- prediction(predicted_probit_correct, test_probit$admit)
perf_probit_correct <- performance(pred_probit_correct,'auc')
auc_probit_correct <- as.numeric(perf_probit_correct@y.values)
auc_probit_correct
```

## The incorrect model: Linearity Assumption

-   Breaking the assumption that the z-score of the outcome and the predictor/s have a linear relationship

```{r}
my_data$probit_incorrect <- my_data$probit_correct + my_data$gpa^4
ggplot(my_data, aes(y = probit_incorrect, x = gpa))+
    geom_point(size = 0.5, alpha = 0.5) +
    geom_smooth(method = "loess") +
    theme_bw() 
```

:::

## The incorrect model: AUC

```{r, echo=T, eval=T}
# defining new 'admit' values depending on how we manipulated the probit values to induce a non-linear association between z-score of the outcome and predictor
my_data$admit_wrong <- ifelse(pnorm(my_data$probit_incorrect) < 0.5, 0, 1)
fit_probit_incorrect <- glm(admit_wrong ~ gpa + gre, data = my_data,
                 family = binomial(link = 'probit'))
predicted_probit_incorrect <- predict(fit_probit_incorrect, test_probit, type='response')
pred_probit_incorrect  <- prediction(predicted_probit_incorrect, test_probit$admit)
perf_probit_incorrect <- performance(pred_probit_incorrect,'auc')
auc_probit_incorrect <- as.numeric(perf_probit_incorrect@y.values)
auc_probit_incorrect
```

# Thank You

## Guiding Question:

Does building your regression function help you gain a deep understanding of how regression models work under the hood?

## Goals of the Project:

When would you use the regression model you chose? When is it preferable to use and when not?

Estimate coefficients of the predictors, estimate their standard errors, and calculate the p-values of said models - show we get similar output as R gives us

How does it work?

Check the assumptions of each model - probably see residuals?

Discuss the implications of breaking assumptions for the models

What are the applications of the model?

Discuss obstacles we are going through/went through + how we plan to/how we did solve them
