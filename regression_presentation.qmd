---
title: "DIY Regression"
author: "Saumya Seth, Jasmine Siswandjo"
format: revealjs
incremental: true
editor: visual
---

```{r}
library(tidyverse)
library(alr4)
```

# DIY Regression

Fall 2023

Statistical Consulting Research Seminar

## Our implementations of the Probit and Zero/Poisson Regression Models

## Guiding Question:

Does building your regression function help you gain a deep understanding of how regression models work under the hood?

## Goals of the Project:

1.  When would you use the regression model you chose? When is it preferable to use and when not?
2.  Estimate coefficients of the predictors, estimate their standard errors, and calculate the p-values of said models - show we get similar output as R gives us
3.  How does it work?
4.  Check the assumptions of each model - probably see residuals?
5.  Discuss the implications of breaking assumptions for the models
6.  What are the applications of the model?
7.  Discuss obstacles we are going through/went through + how we plan to/how we did solve them

## Probit regression

#### 1. When is it used?

-   When the outcome is binary - thus, it is mainly used for classification problems

-   When the outcome is categorical with more than 2 categories, we use the multinomial probit regression

-   Since we know that logistic regression is also helpful in predicting binary outcomes, what is the difference between the two?

#### Logit v/s Probit

-   They give almost identical results - they just have different link functions (more on this later)

$$
\text{If z is the latent variable that we do not oberve, then,}\\
\text{Logit has the link function:}\\
z = log(p/(1-p))\\
\text{Probit has the link function:}\\
z = \phi^{-1}(p) \text{ where } \phi \text{ the cumulative normal probability density function}
$$

-   The decision to chose one over the other is discipline-dependent, and it is said that logit is better when you have extreme independent variables (where one particular small or large value will overwhelmingly determine if your outcome is 0 or 1 - overriding the effect of most other variables)

-   There is no 'right' answer to this debate

#### 2. Our Implementation

A snippet of the main parts of the implementation:

```{r}
source("./probit.R")
```

1.  Define the predictor variables
2.  Define the outcome variable
3.  Define the log likelihood function - this will help estimate our betas (regression coefficients)
4.  Define an initial guess to update later in order to maximize the log likelihood
5.  Iterate over the guesses using optim to find the parameter which maximizes the log likelihood

```{r, echo=T, eval=F}
probit_regression <- function(data, x1, x2, ..., y) {
  # predictors
  X <-
    matrix(c(rep(1, n), x_parameters),
           nrow = n,
           ncol = ncol(data))
  # outcome
  Y <- matrix(y, nrow = n, ncol = 1)
  # defining log likelhood function (-ve)
  probit.loglikelihood <- function(beta, X, Y) {
    eta <- X %*% beta
    p <- pnorm(eta)
    loglikelihood <- -sum((1 - Y) * log(1 - p) + Y * log(p))
    return(loglikelihood)
  }
  # define an initial guess
  initial_guess <- matrix(0, nrow = ncol(data), ncol = 1)
  # iterate to find the maximum log likelihood
  result <-
    optim(
      initial_guess,
      probit.loglikelihood,
      X = X,
      Y = Y,
      method = 'BFGS'
    )
}
```

## Negative Binomial regression

## Logistic regression

```{r}
source("./logistic.R")
```

```{r echo=T}
optim_logistic <- function(beta, X, Y) {
  beta <- as.matrix(beta, nrow = 4)
  pi <- plogis(X %*% beta)
  loglikelihood <- -sum(Y * log(pi) + (1 - Y) * log(1 - pi))
  return(loglikelihood)
}
```

------------------------------------------------------------------------

```{r echo=T}
sim_data <- data.frame(
  x1 = rnorm(100, 2, 1), 
  x2 = rnorm(100, 4, 1), 
  x3 = rnorm(100, 6, 1))

sim_data$y <- rbinom(
  100, 
  size = 1, 
  prob = plogis(-1 + sim_data$x1 + sim_data$x2 - 0.5 * sim_data$x3)
)


```

## Output from GLM

```{r}
fit_sim_data <- glm(y ~ x1 + x2 + x3, data = sim_data, family = binomial)
t(summary(fit_sim_data)$coef)[1:2, ]
```

## Output from my logistic function

```{r}
logistic_function(fn_formula = "y ~ x1 + x2 + x3", data = sim_data)
```

## Also works for categorical variables

```{r}
Donner$survived <- Donner$y == "survived"
fit_Donner <- glm(survived ~ age + sex + status, data = Donner, family = "binomial")
```

```{r echo=T}
summary(fit_Donner)$coef
logistic_function(fn_formula = "survived ~ age + sex + status", data = Donner)
```

## Poisson regression

## Zero-inflated Poisson regression

