{
  "hash": "f24f18d2e768548ed04e6d53ea322ba0",
  "result": {
    "markdown": "---\ntitle: \"Linear\"\n---\n\n::: {.cell}\n\n:::\n\n\n## Introduction\n\nLinear Regression is one of the simplest regressions out there. In predicting an outcome from various covariate(s), it creates the 'best-fitting' line to the data that we observe to create a model - in that it predicts values on the line when given specific values of the covariates.\n\n## Uses\n\nLinear Regression is used across various fields. It is a model which has high bias and low variance. This means that even though it may not fit the data observed in the most optimal way (in that it may not be able to capture complexities in the data), it is not that sensitive to changes in the training data, which can make it more stable when dealing with small fluctuations or noise in the data set. Linear Regression can be used for predicting continuous, categorical, and even binary outcomes (as is often done in Causal Inference).\n\n## Assumptions\n\n-   The predictors and the outcome are linearly related to one another\n-   The errors are normally distributed and are independent of one another\n-   The errors are homoscedastic\n\n## Our Linear Regression Implementation\n\nOur Linear Regression implementation: (Note that we use bootstrapping to estimate standard errors)\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nlinear_regression <- function(data, ..., y) {\n  x_parameters <- c(...)\n  n <- nrow(data)\n  # defining the predictor matrix\n  X <-\n    matrix(c(rep(1, n), x_parameters),\n      nrow = n,\n      ncol = ncol(data)\n    )\n  # defining the outcome matrix\n  Y <- matrix(y, nrow = n, ncol = 1)\n  # solving for the beta coefficients\n  beta <- solve(t(X) %*% X) %*% t(X) %*% Y\n  # creating a vector 'estimate' for the beta coefficients\n  estimate <- c()\n  for (i in 1:ncol(X)) {\n    estimate[i] <- beta[i]\n  }\n  # bootstrapping to estimate the standard errors\n  num_bootstraps <- 10000\n  bootstrap_betas <-\n    matrix(0, nrow = num_bootstraps, ncol = ncol(data))\n  for (i in 1:num_bootstraps) {\n    sample_indices <- sample(nrow(data), replace = TRUE)\n    bootstrap_data <- data[sample_indices, ]\n    bootstrap_X <-\n      as.matrix(cbind(1, bootstrap_data[, 1:(ncol(bootstrap_data) - 1)]))\n    bootstrap_Y <- as.matrix(bootstrap_data$y, ncol = 1)\n    bootstrap_beta <-\n      solve(t(bootstrap_X) %*% bootstrap_X) %*% t(bootstrap_X) %*% bootstrap_Y\n    bootstrap_betas[i, ] <- bootstrap_beta\n  }\n  # finding the standard deviation of the bootstrapped betas to find the\n  # standard error of the coefficients\n  se <- c()\n  for (i in 1:ncol(X)) {\n    se[i] <- apply(bootstrap_betas, 2, sd)[i]\n  }\n  # calculating the t-statistic\n  t <- estimate / se\n  # defining the degrees of freedom\n  df <- nrow(X) - ncol(X)\n  # calculating the p-value\n  p <- 2 * pt(t, df, lower = F)\n  # calculating the residuals\n  resid <- Y - X %*% beta\n  residual <- sqrt(mean((resid)^2))\n  # defining the row names of the output data frame\n  rownames <- c()\n  for (i in 1:((ncol(X)) - 1)) {\n    rownames[i] <- i\n  }\n  test <- list(\n    plot(resid, main = \"Residual Plot to test homoscedasticity of errors\", ylim = c(-10, 10)),\n    qqnorm(resid, main = \"Q-Q plot to test normality of errors\"),\n    pairs(data, main = \"Assessing Linearity of\\n Predictors with Outcome\")\n  )\n  impl <- data.frame(\n    Estimate = estimate,\n    Std.Error = se,\n    t.value = t,\n    p.value = p,\n    Residual = c(residual, rep(NA, ncol(X) - 1)),\n    DegOfFreedom = c(df, rep(NA, ncol(X) - 1)),\n    row.names = c(\"(Intercept)\", paste0(rep(\"x\", ncol(\n      X\n    ) - 1), rownames))\n  )\n  # returning a data frame akin to the lm output\n  return(list(test, impl))\n}\n```\n:::\n\n\nCreating a test data set which meets all Linear Regression assumptions to check if our function works.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\ntest_linear_regression_data <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- rnorm(100, mean = 0, sd = 1) # errors are homoscedastic\ntest_linear_regression_data$y <-\n  2 * test_linear_regression_data$x1 +\n  0.2 * test_linear_regression_data$x2 + error\n\nplot(test_linear_regression_data$x1, test_linear_regression_data$y,\n  xlab = \"x1\", ylab = \"y\",\n  main = \"Outcome is linear to x1\"\n)\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-3-1.png){fig-align='left' width=8.5in}\n:::\n\n```{.r .cell-code}\nplot(test_linear_regression_data$x2, test_linear_regression_data$y,\n  xlab = \"x2\", ylab = \"y\",\n  main = \"Outcome is linear to x2 (it is not apparent in this plot but our data structure captures this relationship)\", cex.main = 0.6\n)\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-3-2.png){fig-align='left' width=8.5in}\n:::\n\n```{.r .cell-code}\nplot(density(error), main = \"Errors are normally distributed with mean 0\")\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-3-3.png){fig-align='left' width=8.5in}\n:::\n\n```{.r .cell-code}\nplot(error,\n  ylab = \"residuals\", main = \"Residuals are homoscedastic\", ylim = c(-3, 3)\n)\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-3-4.png){fig-align='left' width=8.5in}\n:::\n:::\n\n\n## Testing Assumptions for Linear Regression\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nour_implementation <- linear_regression(\n  test_linear_regression_data,\n  test_linear_regression_data$x1,\n  test_linear_regression_data$x2,\n  y = test_linear_regression_data$y\n)[[2]]\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-4-1.png){fig-align='left' width=8.5in}\n:::\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-4-2.png){fig-align='left' width=8.5in}\n:::\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-4-3.png){fig-align='left' width=8.5in}\n:::\n\n```{.r .cell-code}\nour_implementation\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Estimate  Std.Error   t.value      p.value  Residual DegOfFreedom\n(Intercept) 0.4679943 0.31344739  1.493055 1.386684e-01 0.9369198           97\nx1          1.9334142 0.05792076 33.380331 5.582525e-55        NA           NA\nx2          0.2119056 0.05038448  4.205772 5.802633e-05        NA           NA\n```\n:::\n:::\n\n\nComparing our output to R's output.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nr_implementation <-\n  summary(lm(y ~ x1 + x2, data = test_linear_regression_data))\nr_implementation\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = test_linear_regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8730 -0.6607 -0.1245  0.6214  2.0798 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.46799    0.28753   1.628    0.107    \nx1           1.93341    0.05243  36.873  < 2e-16 ***\nx2           0.21191    0.04950   4.281 4.37e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9513 on 97 degrees of freedom\nMultiple R-squared:  0.9337,\tAdjusted R-squared:  0.9323 \nF-statistic: 682.8 on 2 and 97 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nWe note that the results are similar.\n\nWe followed all assumptions of Linear Regression in regressing y on x1 and x2 using the test_linear_regression_data data set. We will compare the residual of this regression to that of all the others where assumptions will be broken.\n\nThe residual for where all assumptions are met:\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nour_implementation$Residual[1] # a small residual here\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9369198\n```\n:::\n:::\n\n\n## Breaking Assumptions\n\n### Breaking the assumption of the predictors and outcome following a linear relationship\n\nCreating a data set where, if we apply linear regression, this assumption will be broken.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\ntest_linear_regression_data_not_linear <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- rnorm(100, mean = 0, sd = 1)\ntest_linear_regression_data_not_linear$y <-\n  2 * test_linear_regression_data_not_linear$x1^2 + 0.2 *\n    test_linear_regression_data_not_linear$x2^2 + error\n\nplot(test_linear_regression_data_not_linear$x1, test_linear_regression_data_not_linear$y,\n  xlab = \"x1\", ylab = \"y\",\n  main = \"Outcome is not linear to x1\"\n)\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-7-1.png){fig-align='left' width=8.5in}\n:::\n\n```{.r .cell-code}\nplot(test_linear_regression_data_not_linear$x2, test_linear_regression_data_not_linear$y,\n  xlab = \"x2\", ylab = \"y\",\n  main = \"Outcome is not linear to x2\"\n)\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-7-2.png){fig-align='left' width=8.5in}\n:::\n:::\n\n\nUsing our implementation of Linear Regression to fit the model.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nour_implementation_not_linear <- linear_regression(\n  test_linear_regression_data_not_linear,\n  test_linear_regression_data_not_linear$x1,\n  test_linear_regression_data_not_linear$x2,\n  y = test_linear_regression_data_not_linear$y\n)[[2]]\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-8-1.png){fig-align='left' width=8.5in}\n:::\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-8-2.png){fig-align='left' width=8.5in}\n:::\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-8-3.png){fig-align='left' width=8.5in}\n:::\n\n```{.r .cell-code}\nour_implementation_not_linear$Residual[1] # a higher residual here\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10.22817\n```\n:::\n:::\n\n\nWe note that linear regression is not performing as well in this case.\n\n### Breaking the assumption of the errors being normally distributed\n\nCreating a data set where, if we apply linear regression, this assumption will be broken.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\ntest_linear_regression_data_not_normally_dist <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- runif(100, min = 0, max = 5)\ntest_linear_regression_data_not_normally_dist$y <-\n  2 * test_linear_regression_data_not_normally_dist$x1 + 0.2 *\n    test_linear_regression_data_not_normally_dist$x2 + error\n\nplot(density(error), main = \"Errors are not normally distributed\")\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-9-1.png){fig-align='left' width=8.5in}\n:::\n:::\n\n\nUsing our implementation of lm to fit the model.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nour_implementation_not_normally_dist <- linear_regression(\n  test_linear_regression_data_not_normally_dist,\n  test_linear_regression_data_not_normally_dist$x1,\n  test_linear_regression_data_not_normally_dist$x2,\n  y = test_linear_regression_data_not_normally_dist$y\n)[[2]]\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-10-1.png){fig-align='left' width=8.5in}\n:::\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-10-2.png){fig-align='left' width=8.5in}\n:::\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-10-3.png){fig-align='left' width=8.5in}\n:::\n\n```{.r .cell-code}\nour_implementation_not_normally_dist$Residual[1] # a higher residual here\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.414274\n```\n:::\n:::\n\n\nWe note that linear regression is not performing as well in this case.\n\n### Breaking the assumption of the errors being homoscedastic\n\nCreating a data set where, if we apply linear regression, this assumption will be broken.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\ntest_linear_regression_data_not_homoscedastic <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- c(\n  rnorm(50, mean = 0, sd = 1),\n  rnorm(50, mean = 0, sd = 10)\n)\ntest_linear_regression_data_not_homoscedastic$y <-\n  2 * test_linear_regression_data_not_homoscedastic$x1 + 0.2 *\n    test_linear_regression_data_not_homoscedastic$x2 + error\n\nplot(error,\n  ylab = \"error\", main = \"Residuals are not homoscedastic\", ylim = c(-20, 20)\n)\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-11-1.png){fig-align='left' width=8.5in}\n:::\n:::\n\n\nUsing our implementation of lm to fit the model.\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nour_implementation_not_homoscedastic <- linear_regression(\n  test_linear_regression_data_not_homoscedastic,\n  test_linear_regression_data_not_homoscedastic$x1,\n  test_linear_regression_data_not_homoscedastic$x2,\n  y = test_linear_regression_data_not_homoscedastic$y\n)[[2]]\n```\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-12-1.png){fig-align='left' width=8.5in}\n:::\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-12-2.png){fig-align='left' width=8.5in}\n:::\n\n::: {.cell-output-display}\n![](linear_files/figure-html/unnamed-chunk-12-3.png){fig-align='left' width=8.5in}\n:::\n\n```{.r .cell-code}\nour_implementation_not_homoscedastic$Residual[1] # a higher residual here\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.419781\n```\n:::\n:::\n\n\nWe note that linear regression is not performing as well in this case.\n\n## Comparing residuals when all assumptions were met versus not\n\n\n::: {.cell layout-align=\"left\"}\n\n```{.r .cell-code}\nresidual_comparison <-\n  t(\n    data.frame(\n      resid_all_assumptions_met = our_implementation$Residual[1],\n      resid_not_linear = our_implementation_not_linear$Residual[1],\n      resid_not_normally_dist = our_implementation_not_normally_dist$Residual[1],\n      resid_not_homoscedastic = our_implementation_not_homoscedastic$Residual[1]\n    )\n  )\nrow.names(residual_comparison) <- c(\n  \"All assumptions met\",\n  \"Linearity assumption violated\",\n  \"Normality assumption violated\",\n  \"Homoscedasticity assumption violated\"\n)\ncolnames(residual_comparison) <- \"Residuals\"\nresidual_comparison\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      Residuals\nAll assumptions met                   0.9369198\nLinearity assumption violated        10.2281685\nNormality assumption violated         1.4142737\nHomoscedasticity assumption violated  6.4197814\n```\n:::\n:::\n\n\n## Conclusion\n\nThe implementation of Linear Regression where all assumptions are met performs the best; i.e. it gives us predictions which are closest to the true outcome values. From the residual comparison, we also note that applying linear regression to data that aren't linear can be especially worrisome.\n",
    "supporting": [
      "linear_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}