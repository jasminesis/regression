{"title":"Logistic","markdown":{"yaml":{"title":"Logistic"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r, message=F, echo=F}\nlibrary(knitr)\nopts_chunk$set(fig.width = 5, fig.height = 4, fig.align = \"left\", out.width = \"8.5in\")\n\nset.seed(123)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(alr4)\nlibrary(pscl)\nlibrary(glmbb) # for crabs data\nlibrary(kableExtra)\nlibrary(lmtest)\n```\n\n\n\nLogistic regression is used when the outcome variable is discrete and binary, which is called classification. Multinomial logistic regression can classify observations into more than two categories, but we are only doing simple logistic regression here, with two categories. We use the inverse logit function to model the probability that $Y_i = 1$.\n\n$$\nlogit^{-1}(x)=\\frac{e^x}{1+e^x} \\\\\nPr(y_i=1) = logit^{-1}(X_i\\beta)\n$$`plogis` is the invlogit function.\n\n## Uses\n\nLogistic regression is good for when covariates are continuous, as the outcome variables are bounded between 0 and 1, through the logit link.\n\n## Assumptions\n\n1.  For binary logistic regression, that outcome variables are binary\n2.  Independence of errors\n3.  Linear relationship between the outcome variable and log odds of the predictor variables\n4.  No multicollinearity\n\n## Our Logistic Regression Implementation\n\n```{r}\nlibrary(alr4)\n# invlogit <- plogis\n\nlogistic_function <- function(fn_formula, data, predict = F) {\n  number_omitted <- nrow(data) - nrow(na.omit(data))\n  data <- na.omit(data)\n\n  vars <- all.vars(as.formula(fn_formula))\n  y_name <- vars[1]\n  x_name <- vars[2:length(vars)]\n  n <- nrow(data)\n  Y <- matrix(data[, y_name], nrow = n, ncol = 1)\n  X <- matrix(cbind(rep(1, n)))\n\n  # take in categorical data\n  var_names <- vector(\"character\")\n  for (i in x_name) {\n    if (suppressWarnings(all(!is.na(as.numeric(as.character(data[, i])))))) {\n      X <- cbind(X, as.numeric(as.character(data[, i])))\n      var_names <- c(var_names, i)\n    } else {\n      categories <- sort(unique(data[, i]))\n      for (j in categories[2:length(categories)]) {\n        new_col_name <- paste0(i, j)\n        new_col <- ifelse(data[, i] == j, 1, 0)\n        X <- cbind(X, new_col)\n        var_names <- c(var_names, new_col_name)\n      }\n    }\n  }\n  optim_logistic <- function(beta, X, Y) {\n    beta <- as.matrix(beta, nrow = 4)\n    pi <- plogis(X %*% beta)\n    loglikelihood <- -sum(Y * log(pi) + (1 - Y) * log(1 - pi))\n    return(loglikelihood)\n  }\n  result <- optim(par = rep(0, ncol(X)), fn = optim_logistic, X = X, Y = Y, hessian = T)\n  OI <- solve(result$hessian)\n  se <- sqrt(diag(OI))\n  t_statistic <- result$par / se\n  df <- nrow(X) - ncol(X)\n  p_value <- 2 * pnorm(-1 * abs(t_statistic))\n  # https://stats.stackexchange.com/questions/52475/how-are-the-p-values-of-the-glm-in-r-calculated\n\n  coef <- rbind(result$par, se, t_statistic, p_value)\n  colnames(coef) <- c(\"(Intercept)\", var_names)\n  rownames(coef) <- c(\"Estimate\", \"Std. Error\", \"z value\", \"p value\")\n  coef <- t(coef)\n\n  b_hat <- result$par\n  predictions <- plogis(X %*% b_hat)\n\n  if (predict) {\n    return(predictions)\n  } else {\n    return(coef)\n  }\n}\n```\n\nCreating testing data set with a single predictor variable to compare our implementation with `glm` logistic function\n\n```{r}\n# create fake data\nx1 <- rnorm(100, 2, 1)\nprob <- plogis(-1 + 0.5 * x1)\ny <- rbinom(100, 1, prob)\nsim_data <- data.frame(y, x1)\n\n# compare DIY logistic function with glm\nfit_sim_data_1 <- glm(y ~ x1, data = sim_data, family = binomial)\nsummary(fit_sim_data_1)$coef\nlogistic_function(fn_formula = \"y ~ x1\", data = sim_data)\n\n# checking for linear relationship\nplot(sim_data$x1, prob,\n  main = \"The log odds of y and x1 have a linear relationship\", cex.main = 0.6,\n  xlab = \"x1\", ylab = \"y\"\n)\n\n# check for correlation of residuals vs fit\nplot(fit_sim_data_1, which = 1)\n```\n\nCreating testing data set with multiple covariates to compare our implementation with `glm` logistic function\n\n```{r}\n# create fake data with multiple x's\nx1 <- rnorm(100, 2, 1)\nx2 <- rnorm(100, 4, 1)\nx3 <- rnorm(100, 6, 1)\nprob <- plogis(-1 + x1 + x2 - 0.5 * x3)\ny <- rbinom(100, 1, prob)\nsim_data <- data.frame(y, x1, x2, x3)\n\n# compare DIY logistic function with glm\nfit_sim_data <- glm(y ~ x1 + x2 + x3, data = sim_data, family = binomial)\nsummary(fit_sim_data)$coef\nlogistic_function(fn_formula = \"y ~ x1 + x2 + x3\", data = sim_data)\n```\n\nUsing the alr4 Donner data to test categorical data, and compare our implementation with `glm` logistic function\n\n```{r}\nDonner$survived <- Donner$y == \"survived\"\nfit_Donner <- glm(survived ~ age + sex + status, data = Donner, family = \"binomial\")\nsummary(fit_Donner)$coef\nlogistic_function(fn_formula = \"survived ~ age + sex + status\", data = Donner)\n```\n\n**Interpretation of the coefficients**\n\nA 1-unit difference in age corresponds to -0.02 in the logit probability of having survived in the Donner party, or a multiplicative change of $e^{-0.0283}=0.972$ in the odds of surviving.\n\n```{r}\nggplot(Donner) +\n  geom_jitter(aes(x = age, y = survived, color = survived)) +\n  facet_wrap(vars(status)) +\n  ggtitle(\"Younger people generally had a higher chance of surviving\")\n```\n\nShow that our implementation of logistic regression can also make predictions\n\n```{r}\nidx <- sample(1:nrow(Donner), 5)\np1 <- logistic_function(fn_formula = \"survived ~ age + sex + status\", data = Donner, predict = T)[idx, ]\np2 <- predict(fit_Donner, type = \"response\")[idx]\ncompare_predict_data <- data.frame(p1, p2)\ncolnames(compare_predict_data) <- c(\"Our implementation\", \"GLM\")\n\nkable(compare_predict_data, digits = 3, caption = \"Comparison of logistic prediction\", booktabs = TRUE, valign = \"t\") |> kable_styling(latex_options = \"HOLD_position\")\n```\n\n## Function to check assumptions\n\n```{r}\ntest_logistic_assumptions <- function(fn_formula, data) {\n  n <- nrow(data)\n  vars <- all.vars(as.formula(fn_formula))\n  y_name <- vars[1]\n  Y <- data[, y_name]\n\n  # outcome variables are binary\n  if (length(unique(Y)) == 2) {\n    assp_1 <- paste(\"Binary outcomes assumption is met.\")\n  } else {\n    return(paste(\"Binary outcomes assumption is not satisfied. There are\", length(unique(Y)), \"outcomes.\"))\n  }\n\n  x_name <- vars[2:length(vars)]\n  X <- data[, x_name]\n  preds <- logistic_function(fn_formula = fn_formula, data = data, predict = T) # predictions are in probability\n  logit_vals <- log(preds / (1 - preds))\n  plot_data <- data.frame(logit_vals, X) |> gather(key = \"predictors\", value = \"predictor_value\", -logit_vals)\n\n  # Linear relationship between the outcome variable and log odds of the predictor variables\n  assp_2 <- ggplot(plot_data, aes(logit_vals, predictor_value)) +\n    geom_point(size = 0.5, alpha = 0.5) +\n    geom_smooth(method = \"loess\") +\n    theme_bw() +\n    facet_wrap(~predictors, scales = \"free_y\")\n\n  # Independence of errors\n  # plot residuals vs fits\n  model <- glm(fn_formula, data = data, family = binomial)\n  assp_3 <- plot(model, which = 1)\n\n  # No multicollinearity\n  assp_4 <- cor(data[, -1])\n\n  predicted.values <- ifelse(preds >= 0.5, 1, 0)\n  check_perf <- data.frame(Y, predicted.values) |> mutate(correct = Y == predicted.values)\n  check_perf <- paste0(mean(check_perf$correct) * 100, \"% classified correctly\")\n  return(list(assp_1, assp_2, print(assp_3), assp_4, check_perf))\n}\n\nn <- 200\nx1 <- rnorm(n, 2, 1)\nx2 <- rnorm(n, 4, 1)\nprob <- plogis(-1 + 1.2 * x1 - 0.5 * x2)\nhist(prob)\ny <- rbinom(n, 1, prob)\nsim_data <- data.frame(y, x1, x2)\n\ntest_logistic_assumptions(fn_formula = \"y ~ x1 + x2\", data = sim_data)\n```\n\n1.  For binary logistic regression, that outcome variables are binary\n\n    Check how many unique outcome variables there are.\n\n2.  Independence of errors\n\n    Check residual plots.\n\n3.  Linear relationship between the outcome variable and log odds of the predictor variables\n\n    Check scatterplots of log odds vs predictor variables to see that there is an approximately linear relationship.\n\n4.  No multicollinearity\n\n    Look at the correlation matrix, generally any value over 0.9 is problematic.\n\n## Breaking assumptions\n\n#### 1. For binary logistic regression, that outcome variables are binary\n\n```{r}\nn <- 100\nx1 <- rnorm(n, 2, 1)\nprob <- plogis(-1 + 0.8 * x1)\ny <- rpois(n, prob)\nbad_data <- data.frame(y, x1)\n\ntest_logistic_assumptions(fn_formula = \"y ~ x1\", data = bad_data)\n```\n\n#### 2. Independence of errors\n\n```{r}\nn <- 100\nx1 <- rnorm(n, 2, 1)\nx2 <- rnorm(n, 0, 1)\nprob <- plogis(-1.2 + 0.4 * x1 + 0.3 * x2 + rbeta(n, 2, 2))\nhist(prob)\ny <- rbinom(n, 1, prob)\nbad_data <- data.frame(y, x1, x2)\n\ntest_logistic_assumptions(fn_formula = \"y ~ x1 + x2\", data = bad_data)\n```\n\nI used `rbeta` to introduce more error for the middle of the probabilities, which shows in the residual plot, where it looks like there is a peak in the fitted line around 0.\n\n#### 3. Linear relationship between the outcome variable and log odds of the predictor variables\n\n```{r}\nn <- 100\nx1 <- rnorm(n, 0, 1)\nx2 <- rnorm(n, 2, 1)\nx3 <- rnorm(n, -2, 1)\nprob <- plogis(-1 + x1 * x2 + 1.3 * x2 - 0.5 * x3^2)\nhist(prob)\ny <- rbinom(n, 1, prob)\nbad_data <- data.frame(y, x1, x2, x3)\n\ntest_logistic_assumptions(fn_formula = \"y ~ x1 + x2 + x3\", data = bad_data)\n```\n\nFor the probability used in the DGP, it is no longer a linear relationship of $X_i\\beta$ but one that involves interactions and squared variables. The non-linear relationship is visible in the plots of the outcome variable vs log odds of each predictor variable.\n\n#### 4. No multicollinearity\n\n```{r}\nn <- 100\nx1 <- rnorm(n, 0, 1)\nx2 <- rnorm(n, x1, 0.2)\nprob <- plogis(-1 + x1 + 0.5 * x2)\nhist(prob)\ny <- rbinom(n, 1, prob)\nbad_data <- data.frame(y, x1, x2)\n\ntest_logistic_assumptions(fn_formula = \"y ~ x1 + x2\", data = bad_data)\n```\n\nThe DGP involves $x2$ being created from an `rnorm` around the mean of $x1$, and a small SD so that it is even more correlated. The correlation matrix shows a problematically high correlation. The model still classifies fairly well, but what happens with multicollinearity is that it becomes difficult for the model to estimate the relationship between each predictor variable and the outcome variable independently because the predictor variables tend to change in unison. The p-values are also less trustworthy.\n\n## Next Steps\n\n-   Instead of just using maximum likelihood, we could try using iteratively reweighted least squares or Newton-Ralphson.\n\n## Conclusion\n\nBreaking certain assumptions do not make the logistic model classify worse. Independence of errors seems to be the worst case, but the rest do not change the correct classification rate much.\n","srcMarkdownNoYaml":"\n\n```{r, message=F, echo=F}\nlibrary(knitr)\nopts_chunk$set(fig.width = 5, fig.height = 4, fig.align = \"left\", out.width = \"8.5in\")\n\nset.seed(123)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(alr4)\nlibrary(pscl)\nlibrary(glmbb) # for crabs data\nlibrary(kableExtra)\nlibrary(lmtest)\n```\n\n\n## Introduction\n\nLogistic regression is used when the outcome variable is discrete and binary, which is called classification. Multinomial logistic regression can classify observations into more than two categories, but we are only doing simple logistic regression here, with two categories. We use the inverse logit function to model the probability that $Y_i = 1$.\n\n$$\nlogit^{-1}(x)=\\frac{e^x}{1+e^x} \\\\\nPr(y_i=1) = logit^{-1}(X_i\\beta)\n$$`plogis` is the invlogit function.\n\n## Uses\n\nLogistic regression is good for when covariates are continuous, as the outcome variables are bounded between 0 and 1, through the logit link.\n\n## Assumptions\n\n1.  For binary logistic regression, that outcome variables are binary\n2.  Independence of errors\n3.  Linear relationship between the outcome variable and log odds of the predictor variables\n4.  No multicollinearity\n\n## Our Logistic Regression Implementation\n\n```{r}\nlibrary(alr4)\n# invlogit <- plogis\n\nlogistic_function <- function(fn_formula, data, predict = F) {\n  number_omitted <- nrow(data) - nrow(na.omit(data))\n  data <- na.omit(data)\n\n  vars <- all.vars(as.formula(fn_formula))\n  y_name <- vars[1]\n  x_name <- vars[2:length(vars)]\n  n <- nrow(data)\n  Y <- matrix(data[, y_name], nrow = n, ncol = 1)\n  X <- matrix(cbind(rep(1, n)))\n\n  # take in categorical data\n  var_names <- vector(\"character\")\n  for (i in x_name) {\n    if (suppressWarnings(all(!is.na(as.numeric(as.character(data[, i])))))) {\n      X <- cbind(X, as.numeric(as.character(data[, i])))\n      var_names <- c(var_names, i)\n    } else {\n      categories <- sort(unique(data[, i]))\n      for (j in categories[2:length(categories)]) {\n        new_col_name <- paste0(i, j)\n        new_col <- ifelse(data[, i] == j, 1, 0)\n        X <- cbind(X, new_col)\n        var_names <- c(var_names, new_col_name)\n      }\n    }\n  }\n  optim_logistic <- function(beta, X, Y) {\n    beta <- as.matrix(beta, nrow = 4)\n    pi <- plogis(X %*% beta)\n    loglikelihood <- -sum(Y * log(pi) + (1 - Y) * log(1 - pi))\n    return(loglikelihood)\n  }\n  result <- optim(par = rep(0, ncol(X)), fn = optim_logistic, X = X, Y = Y, hessian = T)\n  OI <- solve(result$hessian)\n  se <- sqrt(diag(OI))\n  t_statistic <- result$par / se\n  df <- nrow(X) - ncol(X)\n  p_value <- 2 * pnorm(-1 * abs(t_statistic))\n  # https://stats.stackexchange.com/questions/52475/how-are-the-p-values-of-the-glm-in-r-calculated\n\n  coef <- rbind(result$par, se, t_statistic, p_value)\n  colnames(coef) <- c(\"(Intercept)\", var_names)\n  rownames(coef) <- c(\"Estimate\", \"Std. Error\", \"z value\", \"p value\")\n  coef <- t(coef)\n\n  b_hat <- result$par\n  predictions <- plogis(X %*% b_hat)\n\n  if (predict) {\n    return(predictions)\n  } else {\n    return(coef)\n  }\n}\n```\n\nCreating testing data set with a single predictor variable to compare our implementation with `glm` logistic function\n\n```{r}\n# create fake data\nx1 <- rnorm(100, 2, 1)\nprob <- plogis(-1 + 0.5 * x1)\ny <- rbinom(100, 1, prob)\nsim_data <- data.frame(y, x1)\n\n# compare DIY logistic function with glm\nfit_sim_data_1 <- glm(y ~ x1, data = sim_data, family = binomial)\nsummary(fit_sim_data_1)$coef\nlogistic_function(fn_formula = \"y ~ x1\", data = sim_data)\n\n# checking for linear relationship\nplot(sim_data$x1, prob,\n  main = \"The log odds of y and x1 have a linear relationship\", cex.main = 0.6,\n  xlab = \"x1\", ylab = \"y\"\n)\n\n# check for correlation of residuals vs fit\nplot(fit_sim_data_1, which = 1)\n```\n\nCreating testing data set with multiple covariates to compare our implementation with `glm` logistic function\n\n```{r}\n# create fake data with multiple x's\nx1 <- rnorm(100, 2, 1)\nx2 <- rnorm(100, 4, 1)\nx3 <- rnorm(100, 6, 1)\nprob <- plogis(-1 + x1 + x2 - 0.5 * x3)\ny <- rbinom(100, 1, prob)\nsim_data <- data.frame(y, x1, x2, x3)\n\n# compare DIY logistic function with glm\nfit_sim_data <- glm(y ~ x1 + x2 + x3, data = sim_data, family = binomial)\nsummary(fit_sim_data)$coef\nlogistic_function(fn_formula = \"y ~ x1 + x2 + x3\", data = sim_data)\n```\n\nUsing the alr4 Donner data to test categorical data, and compare our implementation with `glm` logistic function\n\n```{r}\nDonner$survived <- Donner$y == \"survived\"\nfit_Donner <- glm(survived ~ age + sex + status, data = Donner, family = \"binomial\")\nsummary(fit_Donner)$coef\nlogistic_function(fn_formula = \"survived ~ age + sex + status\", data = Donner)\n```\n\n**Interpretation of the coefficients**\n\nA 1-unit difference in age corresponds to -0.02 in the logit probability of having survived in the Donner party, or a multiplicative change of $e^{-0.0283}=0.972$ in the odds of surviving.\n\n```{r}\nggplot(Donner) +\n  geom_jitter(aes(x = age, y = survived, color = survived)) +\n  facet_wrap(vars(status)) +\n  ggtitle(\"Younger people generally had a higher chance of surviving\")\n```\n\nShow that our implementation of logistic regression can also make predictions\n\n```{r}\nidx <- sample(1:nrow(Donner), 5)\np1 <- logistic_function(fn_formula = \"survived ~ age + sex + status\", data = Donner, predict = T)[idx, ]\np2 <- predict(fit_Donner, type = \"response\")[idx]\ncompare_predict_data <- data.frame(p1, p2)\ncolnames(compare_predict_data) <- c(\"Our implementation\", \"GLM\")\n\nkable(compare_predict_data, digits = 3, caption = \"Comparison of logistic prediction\", booktabs = TRUE, valign = \"t\") |> kable_styling(latex_options = \"HOLD_position\")\n```\n\n## Function to check assumptions\n\n```{r}\ntest_logistic_assumptions <- function(fn_formula, data) {\n  n <- nrow(data)\n  vars <- all.vars(as.formula(fn_formula))\n  y_name <- vars[1]\n  Y <- data[, y_name]\n\n  # outcome variables are binary\n  if (length(unique(Y)) == 2) {\n    assp_1 <- paste(\"Binary outcomes assumption is met.\")\n  } else {\n    return(paste(\"Binary outcomes assumption is not satisfied. There are\", length(unique(Y)), \"outcomes.\"))\n  }\n\n  x_name <- vars[2:length(vars)]\n  X <- data[, x_name]\n  preds <- logistic_function(fn_formula = fn_formula, data = data, predict = T) # predictions are in probability\n  logit_vals <- log(preds / (1 - preds))\n  plot_data <- data.frame(logit_vals, X) |> gather(key = \"predictors\", value = \"predictor_value\", -logit_vals)\n\n  # Linear relationship between the outcome variable and log odds of the predictor variables\n  assp_2 <- ggplot(plot_data, aes(logit_vals, predictor_value)) +\n    geom_point(size = 0.5, alpha = 0.5) +\n    geom_smooth(method = \"loess\") +\n    theme_bw() +\n    facet_wrap(~predictors, scales = \"free_y\")\n\n  # Independence of errors\n  # plot residuals vs fits\n  model <- glm(fn_formula, data = data, family = binomial)\n  assp_3 <- plot(model, which = 1)\n\n  # No multicollinearity\n  assp_4 <- cor(data[, -1])\n\n  predicted.values <- ifelse(preds >= 0.5, 1, 0)\n  check_perf <- data.frame(Y, predicted.values) |> mutate(correct = Y == predicted.values)\n  check_perf <- paste0(mean(check_perf$correct) * 100, \"% classified correctly\")\n  return(list(assp_1, assp_2, print(assp_3), assp_4, check_perf))\n}\n\nn <- 200\nx1 <- rnorm(n, 2, 1)\nx2 <- rnorm(n, 4, 1)\nprob <- plogis(-1 + 1.2 * x1 - 0.5 * x2)\nhist(prob)\ny <- rbinom(n, 1, prob)\nsim_data <- data.frame(y, x1, x2)\n\ntest_logistic_assumptions(fn_formula = \"y ~ x1 + x2\", data = sim_data)\n```\n\n1.  For binary logistic regression, that outcome variables are binary\n\n    Check how many unique outcome variables there are.\n\n2.  Independence of errors\n\n    Check residual plots.\n\n3.  Linear relationship between the outcome variable and log odds of the predictor variables\n\n    Check scatterplots of log odds vs predictor variables to see that there is an approximately linear relationship.\n\n4.  No multicollinearity\n\n    Look at the correlation matrix, generally any value over 0.9 is problematic.\n\n## Breaking assumptions\n\n#### 1. For binary logistic regression, that outcome variables are binary\n\n```{r}\nn <- 100\nx1 <- rnorm(n, 2, 1)\nprob <- plogis(-1 + 0.8 * x1)\ny <- rpois(n, prob)\nbad_data <- data.frame(y, x1)\n\ntest_logistic_assumptions(fn_formula = \"y ~ x1\", data = bad_data)\n```\n\n#### 2. Independence of errors\n\n```{r}\nn <- 100\nx1 <- rnorm(n, 2, 1)\nx2 <- rnorm(n, 0, 1)\nprob <- plogis(-1.2 + 0.4 * x1 + 0.3 * x2 + rbeta(n, 2, 2))\nhist(prob)\ny <- rbinom(n, 1, prob)\nbad_data <- data.frame(y, x1, x2)\n\ntest_logistic_assumptions(fn_formula = \"y ~ x1 + x2\", data = bad_data)\n```\n\nI used `rbeta` to introduce more error for the middle of the probabilities, which shows in the residual plot, where it looks like there is a peak in the fitted line around 0.\n\n#### 3. Linear relationship between the outcome variable and log odds of the predictor variables\n\n```{r}\nn <- 100\nx1 <- rnorm(n, 0, 1)\nx2 <- rnorm(n, 2, 1)\nx3 <- rnorm(n, -2, 1)\nprob <- plogis(-1 + x1 * x2 + 1.3 * x2 - 0.5 * x3^2)\nhist(prob)\ny <- rbinom(n, 1, prob)\nbad_data <- data.frame(y, x1, x2, x3)\n\ntest_logistic_assumptions(fn_formula = \"y ~ x1 + x2 + x3\", data = bad_data)\n```\n\nFor the probability used in the DGP, it is no longer a linear relationship of $X_i\\beta$ but one that involves interactions and squared variables. The non-linear relationship is visible in the plots of the outcome variable vs log odds of each predictor variable.\n\n#### 4. No multicollinearity\n\n```{r}\nn <- 100\nx1 <- rnorm(n, 0, 1)\nx2 <- rnorm(n, x1, 0.2)\nprob <- plogis(-1 + x1 + 0.5 * x2)\nhist(prob)\ny <- rbinom(n, 1, prob)\nbad_data <- data.frame(y, x1, x2)\n\ntest_logistic_assumptions(fn_formula = \"y ~ x1 + x2\", data = bad_data)\n```\n\nThe DGP involves $x2$ being created from an `rnorm` around the mean of $x1$, and a small SD so that it is even more correlated. The correlation matrix shows a problematically high correlation. The model still classifies fairly well, but what happens with multicollinearity is that it becomes difficult for the model to estimate the relationship between each predictor variable and the outcome variable independently because the predictor variables tend to change in unison. The p-values are also less trustworthy.\n\n## Next Steps\n\n-   Instead of just using maximum likelihood, we could try using iteratively reweighted least squares or Newton-Ralphson.\n\n## Conclusion\n\nBreaking certain assumptions do not make the logistic model classify worse. Independence of errors seems to be the worst case, but the rest do not change the correct classification rate much.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"logistic.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title":"Logistic"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}