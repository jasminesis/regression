{"title":"Linear","markdown":{"yaml":{"title":"Linear"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r, message=F, echo=F}\nlibrary(knitr)\nopts_chunk$set(fig.width = 5, fig.height = 4, fig.align = \"left\", out.width = \"8.5in\")\n\nset.seed(123)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(alr4)\nlibrary(pscl)\nlibrary(kableExtra)\nlibrary(lmtest)\n```\n\n\nLinear Regression is one of the simplest regressions out there. In predicting an outcome from various covariate(s), it creates the 'best-fitting' line to the data that we observe to create a model - in that it predicts values on the line when given specific values of the covariates.\n\n## Uses\n\nLinear Regression is used across various fields. It is a model which has high bias and low variance. This means that even though it may not fit the data observed in the most optimal way (in that it may not be able to capture complexities in the data), it is not that sensitive to changes in the training data, which can make it more stable when dealing with small fluctuations or noise in the data set. Linear Regression can be used for predicting continuous, categorical, and even binary outcomes (as is often done in Causal Inference).\n\n## Assumptions\n\n-   The predictors and the outcome are linearly related to one another\n-   The errors are normally distributed and are independent of one another\n-   The errors are homoscedastic\n\n## Our Linear Regression Implementation\n\nOur Linear Regression implementation: (Note that we use bootstrapping to estimate standard errors)\n\n```{r}\nlinear_regression <- function(data, ..., y) {\n  x_parameters <- c(...)\n  n <- nrow(data)\n  # defining the predictor matrix\n  X <-\n    matrix(c(rep(1, n), x_parameters),\n      nrow = n,\n      ncol = ncol(data)\n    )\n  # defining the outcome matrix\n  Y <- matrix(y, nrow = n, ncol = 1)\n  # solving for the beta coefficients\n  beta <- solve(t(X) %*% X) %*% t(X) %*% Y\n  # creating a vector 'estimate' for the beta coefficients\n  estimate <- c()\n  for (i in 1:ncol(X)) {\n    estimate[i] <- beta[i]\n  }\n  # bootstrapping to estimate the standard errors\n  num_bootstraps <- 10000\n  bootstrap_betas <-\n    matrix(0, nrow = num_bootstraps, ncol = ncol(data))\n  for (i in 1:num_bootstraps) {\n    sample_indices <- sample(nrow(data), replace = TRUE)\n    bootstrap_data <- data[sample_indices, ]\n    bootstrap_X <-\n      as.matrix(cbind(1, bootstrap_data[, 1:(ncol(bootstrap_data) - 1)]))\n    bootstrap_Y <- as.matrix(bootstrap_data$y, ncol = 1)\n    bootstrap_beta <-\n      solve(t(bootstrap_X) %*% bootstrap_X) %*% t(bootstrap_X) %*% bootstrap_Y\n    bootstrap_betas[i, ] <- bootstrap_beta\n  }\n  # finding the standard deviation of the bootstrapped betas to find the\n  # standard error of the coefficients\n  se <- c()\n  for (i in 1:ncol(X)) {\n    se[i] <- apply(bootstrap_betas, 2, sd)[i]\n  }\n  # calculating the t-statistic\n  t <- estimate / se\n  # defining the degrees of freedom\n  df <- nrow(X) - ncol(X)\n  # calculating the p-value\n  p <- 2 * pt(t, df, lower = F)\n  # calculating the residuals\n  resid <- Y - X %*% beta\n  residual <- sqrt(mean((resid)^2))\n  # defining the row names of the output data frame\n  rownames <- c()\n  for (i in 1:((ncol(X)) - 1)) {\n    rownames[i] <- i\n  }\n  test <- list(\n    plot(resid, main = \"Residual Plot to test homoscedasticity of errors\", ylim = c(-10, 10)),\n    qqnorm(resid, main = \"Q-Q plot to test normality of errors\"),\n    pairs(data, main = \"Assessing Linearity of\\n Predictors with Outcome\")\n  )\n  impl <- data.frame(\n    Estimate = estimate,\n    Std.Error = se,\n    t.value = t,\n    p.value = p,\n    Residual = c(residual, rep(NA, ncol(X) - 1)),\n    DegOfFreedom = c(df, rep(NA, ncol(X) - 1)),\n    row.names = c(\"(Intercept)\", paste0(rep(\"x\", ncol(\n      X\n    ) - 1), rownames))\n  )\n  # returning a data frame akin to the lm output\n  return(list(test, impl))\n}\n```\n\nCreating a test data set which meets all Linear Regression assumptions to check if our function works.\n\n```{r}\ntest_linear_regression_data <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- rnorm(100, mean = 0, sd = 1) # errors are homoscedastic\ntest_linear_regression_data$y <-\n  2 * test_linear_regression_data$x1 +\n  0.2 * test_linear_regression_data$x2 + error\n\nplot(test_linear_regression_data$x1, test_linear_regression_data$y,\n  xlab = \"x1\", ylab = \"y\",\n  main = \"Outcome is linear to x1\"\n)\nplot(test_linear_regression_data$x2, test_linear_regression_data$y,\n  xlab = \"x2\", ylab = \"y\",\n  main = \"Outcome is linear to x2 (it is not apparent in this plot but our data structure captures this relationship)\", cex.main = 0.6\n)\nplot(density(error), main = \"Errors are normally distributed with mean 0\")\nplot(error,\n  ylab = \"residuals\", main = \"Residuals are homoscedastic\", ylim = c(-3, 3)\n)\n```\n\n## Testing Assumptions for Linear Regression\n\n```{r}\nour_implementation <- linear_regression(\n  test_linear_regression_data,\n  test_linear_regression_data$x1,\n  test_linear_regression_data$x2,\n  y = test_linear_regression_data$y\n)[[2]]\nour_implementation\n```\n\nComparing our output to R's output.\n\n```{r}\nr_implementation <-\n  summary(lm(y ~ x1 + x2, data = test_linear_regression_data))\nr_implementation\n```\n\nWe note that the results are similar.\n\nWe followed all assumptions of Linear Regression in regressing y on x1 and x2 using the test_linear_regression_data data set. We will compare the residual of this regression to that of all the others where assumptions will be broken.\n\nThe residual for where all assumptions are met:\n\n```{r}\nour_implementation$Residual[1] # a small residual here\n```\n\n## Breaking Assumptions\n\n### Breaking the assumption of the predictors and outcome following a linear relationship\n\nCreating a data set where, if we apply linear regression, this assumption will be broken.\n\n```{r}\ntest_linear_regression_data_not_linear <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- rnorm(100, mean = 0, sd = 1)\ntest_linear_regression_data_not_linear$y <-\n  2 * test_linear_regression_data_not_linear$x1^2 + 0.2 *\n    test_linear_regression_data_not_linear$x2^2 + error\n\nplot(test_linear_regression_data_not_linear$x1, test_linear_regression_data_not_linear$y,\n  xlab = \"x1\", ylab = \"y\",\n  main = \"Outcome is not linear to x1\"\n)\nplot(test_linear_regression_data_not_linear$x2, test_linear_regression_data_not_linear$y,\n  xlab = \"x2\", ylab = \"y\",\n  main = \"Outcome is not linear to x2\"\n)\n```\n\nUsing our implementation of Linear Regression to fit the model.\n\n```{r}\nour_implementation_not_linear <- linear_regression(\n  test_linear_regression_data_not_linear,\n  test_linear_regression_data_not_linear$x1,\n  test_linear_regression_data_not_linear$x2,\n  y = test_linear_regression_data_not_linear$y\n)[[2]]\nour_implementation_not_linear$Residual[1] # a higher residual here\n```\n\nWe note that linear regression is not performing as well in this case.\n\n### Breaking the assumption of the errors being normally distributed\n\nCreating a data set where, if we apply linear regression, this assumption will be broken.\n\n```{r}\ntest_linear_regression_data_not_normally_dist <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- runif(100, min = 0, max = 5)\ntest_linear_regression_data_not_normally_dist$y <-\n  2 * test_linear_regression_data_not_normally_dist$x1 + 0.2 *\n    test_linear_regression_data_not_normally_dist$x2 + error\n\nplot(density(error), main = \"Errors are not normally distributed\")\n```\n\nUsing our implementation of lm to fit the model.\n\n```{r}\nour_implementation_not_normally_dist <- linear_regression(\n  test_linear_regression_data_not_normally_dist,\n  test_linear_regression_data_not_normally_dist$x1,\n  test_linear_regression_data_not_normally_dist$x2,\n  y = test_linear_regression_data_not_normally_dist$y\n)[[2]]\nour_implementation_not_normally_dist$Residual[1] # a higher residual here\n```\n\nWe note that linear regression is not performing as well in this case.\n\n### Breaking the assumption of the errors being homoscedastic\n\nCreating a data set where, if we apply linear regression, this assumption will be broken.\n\n```{r}\ntest_linear_regression_data_not_homoscedastic <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- c(\n  rnorm(50, mean = 0, sd = 1),\n  rnorm(50, mean = 0, sd = 10)\n)\ntest_linear_regression_data_not_homoscedastic$y <-\n  2 * test_linear_regression_data_not_homoscedastic$x1 + 0.2 *\n    test_linear_regression_data_not_homoscedastic$x2 + error\n\nplot(error,\n  ylab = \"error\", main = \"Residuals are not homoscedastic\", ylim = c(-20, 20)\n)\n```\n\nUsing our implementation of lm to fit the model.\n\n```{r}\nour_implementation_not_homoscedastic <- linear_regression(\n  test_linear_regression_data_not_homoscedastic,\n  test_linear_regression_data_not_homoscedastic$x1,\n  test_linear_regression_data_not_homoscedastic$x2,\n  y = test_linear_regression_data_not_homoscedastic$y\n)[[2]]\nour_implementation_not_homoscedastic$Residual[1] # a higher residual here\n```\n\nWe note that linear regression is not performing as well in this case.\n\n## Comparing residuals when all assumptions were met versus not\n\n```{r}\nresidual_comparison <-\n  t(\n    data.frame(\n      resid_all_assumptions_met = our_implementation$Residual[1],\n      resid_not_linear = our_implementation_not_linear$Residual[1],\n      resid_not_normally_dist = our_implementation_not_normally_dist$Residual[1],\n      resid_not_homoscedastic = our_implementation_not_homoscedastic$Residual[1]\n    )\n  )\nrow.names(residual_comparison) <- c(\n  \"All assumptions met\",\n  \"Linearity assumption violated\",\n  \"Normality assumption violated\",\n  \"Homoscedasticity assumption violated\"\n)\ncolnames(residual_comparison) <- \"Residuals\"\nresidual_comparison\n```\n\n## Conclusion\n\nThe implementation of Linear Regression where all assumptions are met performs the best; i.e. it gives us predictions which are closest to the true outcome values. From the residual comparison, we also note that applying linear regression to data that aren't linear can be especially worrisome.\n","srcMarkdownNoYaml":"\n\n```{r, message=F, echo=F}\nlibrary(knitr)\nopts_chunk$set(fig.width = 5, fig.height = 4, fig.align = \"left\", out.width = \"8.5in\")\n\nset.seed(123)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(alr4)\nlibrary(pscl)\nlibrary(kableExtra)\nlibrary(lmtest)\n```\n\n## Introduction\n\nLinear Regression is one of the simplest regressions out there. In predicting an outcome from various covariate(s), it creates the 'best-fitting' line to the data that we observe to create a model - in that it predicts values on the line when given specific values of the covariates.\n\n## Uses\n\nLinear Regression is used across various fields. It is a model which has high bias and low variance. This means that even though it may not fit the data observed in the most optimal way (in that it may not be able to capture complexities in the data), it is not that sensitive to changes in the training data, which can make it more stable when dealing with small fluctuations or noise in the data set. Linear Regression can be used for predicting continuous, categorical, and even binary outcomes (as is often done in Causal Inference).\n\n## Assumptions\n\n-   The predictors and the outcome are linearly related to one another\n-   The errors are normally distributed and are independent of one another\n-   The errors are homoscedastic\n\n## Our Linear Regression Implementation\n\nOur Linear Regression implementation: (Note that we use bootstrapping to estimate standard errors)\n\n```{r}\nlinear_regression <- function(data, ..., y) {\n  x_parameters <- c(...)\n  n <- nrow(data)\n  # defining the predictor matrix\n  X <-\n    matrix(c(rep(1, n), x_parameters),\n      nrow = n,\n      ncol = ncol(data)\n    )\n  # defining the outcome matrix\n  Y <- matrix(y, nrow = n, ncol = 1)\n  # solving for the beta coefficients\n  beta <- solve(t(X) %*% X) %*% t(X) %*% Y\n  # creating a vector 'estimate' for the beta coefficients\n  estimate <- c()\n  for (i in 1:ncol(X)) {\n    estimate[i] <- beta[i]\n  }\n  # bootstrapping to estimate the standard errors\n  num_bootstraps <- 10000\n  bootstrap_betas <-\n    matrix(0, nrow = num_bootstraps, ncol = ncol(data))\n  for (i in 1:num_bootstraps) {\n    sample_indices <- sample(nrow(data), replace = TRUE)\n    bootstrap_data <- data[sample_indices, ]\n    bootstrap_X <-\n      as.matrix(cbind(1, bootstrap_data[, 1:(ncol(bootstrap_data) - 1)]))\n    bootstrap_Y <- as.matrix(bootstrap_data$y, ncol = 1)\n    bootstrap_beta <-\n      solve(t(bootstrap_X) %*% bootstrap_X) %*% t(bootstrap_X) %*% bootstrap_Y\n    bootstrap_betas[i, ] <- bootstrap_beta\n  }\n  # finding the standard deviation of the bootstrapped betas to find the\n  # standard error of the coefficients\n  se <- c()\n  for (i in 1:ncol(X)) {\n    se[i] <- apply(bootstrap_betas, 2, sd)[i]\n  }\n  # calculating the t-statistic\n  t <- estimate / se\n  # defining the degrees of freedom\n  df <- nrow(X) - ncol(X)\n  # calculating the p-value\n  p <- 2 * pt(t, df, lower = F)\n  # calculating the residuals\n  resid <- Y - X %*% beta\n  residual <- sqrt(mean((resid)^2))\n  # defining the row names of the output data frame\n  rownames <- c()\n  for (i in 1:((ncol(X)) - 1)) {\n    rownames[i] <- i\n  }\n  test <- list(\n    plot(resid, main = \"Residual Plot to test homoscedasticity of errors\", ylim = c(-10, 10)),\n    qqnorm(resid, main = \"Q-Q plot to test normality of errors\"),\n    pairs(data, main = \"Assessing Linearity of\\n Predictors with Outcome\")\n  )\n  impl <- data.frame(\n    Estimate = estimate,\n    Std.Error = se,\n    t.value = t,\n    p.value = p,\n    Residual = c(residual, rep(NA, ncol(X) - 1)),\n    DegOfFreedom = c(df, rep(NA, ncol(X) - 1)),\n    row.names = c(\"(Intercept)\", paste0(rep(\"x\", ncol(\n      X\n    ) - 1), rownames))\n  )\n  # returning a data frame akin to the lm output\n  return(list(test, impl))\n}\n```\n\nCreating a test data set which meets all Linear Regression assumptions to check if our function works.\n\n```{r}\ntest_linear_regression_data <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- rnorm(100, mean = 0, sd = 1) # errors are homoscedastic\ntest_linear_regression_data$y <-\n  2 * test_linear_regression_data$x1 +\n  0.2 * test_linear_regression_data$x2 + error\n\nplot(test_linear_regression_data$x1, test_linear_regression_data$y,\n  xlab = \"x1\", ylab = \"y\",\n  main = \"Outcome is linear to x1\"\n)\nplot(test_linear_regression_data$x2, test_linear_regression_data$y,\n  xlab = \"x2\", ylab = \"y\",\n  main = \"Outcome is linear to x2 (it is not apparent in this plot but our data structure captures this relationship)\", cex.main = 0.6\n)\nplot(density(error), main = \"Errors are normally distributed with mean 0\")\nplot(error,\n  ylab = \"residuals\", main = \"Residuals are homoscedastic\", ylim = c(-3, 3)\n)\n```\n\n## Testing Assumptions for Linear Regression\n\n```{r}\nour_implementation <- linear_regression(\n  test_linear_regression_data,\n  test_linear_regression_data$x1,\n  test_linear_regression_data$x2,\n  y = test_linear_regression_data$y\n)[[2]]\nour_implementation\n```\n\nComparing our output to R's output.\n\n```{r}\nr_implementation <-\n  summary(lm(y ~ x1 + x2, data = test_linear_regression_data))\nr_implementation\n```\n\nWe note that the results are similar.\n\nWe followed all assumptions of Linear Regression in regressing y on x1 and x2 using the test_linear_regression_data data set. We will compare the residual of this regression to that of all the others where assumptions will be broken.\n\nThe residual for where all assumptions are met:\n\n```{r}\nour_implementation$Residual[1] # a small residual here\n```\n\n## Breaking Assumptions\n\n### Breaking the assumption of the predictors and outcome following a linear relationship\n\nCreating a data set where, if we apply linear regression, this assumption will be broken.\n\n```{r}\ntest_linear_regression_data_not_linear <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- rnorm(100, mean = 0, sd = 1)\ntest_linear_regression_data_not_linear$y <-\n  2 * test_linear_regression_data_not_linear$x1^2 + 0.2 *\n    test_linear_regression_data_not_linear$x2^2 + error\n\nplot(test_linear_regression_data_not_linear$x1, test_linear_regression_data_not_linear$y,\n  xlab = \"x1\", ylab = \"y\",\n  main = \"Outcome is not linear to x1\"\n)\nplot(test_linear_regression_data_not_linear$x2, test_linear_regression_data_not_linear$y,\n  xlab = \"x2\", ylab = \"y\",\n  main = \"Outcome is not linear to x2\"\n)\n```\n\nUsing our implementation of Linear Regression to fit the model.\n\n```{r}\nour_implementation_not_linear <- linear_regression(\n  test_linear_regression_data_not_linear,\n  test_linear_regression_data_not_linear$x1,\n  test_linear_regression_data_not_linear$x2,\n  y = test_linear_regression_data_not_linear$y\n)[[2]]\nour_implementation_not_linear$Residual[1] # a higher residual here\n```\n\nWe note that linear regression is not performing as well in this case.\n\n### Breaking the assumption of the errors being normally distributed\n\nCreating a data set where, if we apply linear regression, this assumption will be broken.\n\n```{r}\ntest_linear_regression_data_not_normally_dist <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- runif(100, min = 0, max = 5)\ntest_linear_regression_data_not_normally_dist$y <-\n  2 * test_linear_regression_data_not_normally_dist$x1 + 0.2 *\n    test_linear_regression_data_not_normally_dist$x2 + error\n\nplot(density(error), main = \"Errors are not normally distributed\")\n```\n\nUsing our implementation of lm to fit the model.\n\n```{r}\nour_implementation_not_normally_dist <- linear_regression(\n  test_linear_regression_data_not_normally_dist,\n  test_linear_regression_data_not_normally_dist$x1,\n  test_linear_regression_data_not_normally_dist$x2,\n  y = test_linear_regression_data_not_normally_dist$y\n)[[2]]\nour_implementation_not_normally_dist$Residual[1] # a higher residual here\n```\n\nWe note that linear regression is not performing as well in this case.\n\n### Breaking the assumption of the errors being homoscedastic\n\nCreating a data set where, if we apply linear regression, this assumption will be broken.\n\n```{r}\ntest_linear_regression_data_not_homoscedastic <-\n  data.frame(\n    x1 = rnorm(100, mean = 5, sd = 2),\n    x2 = rnorm(100, mean = 0, sd = 2)\n  )\nerror <- c(\n  rnorm(50, mean = 0, sd = 1),\n  rnorm(50, mean = 0, sd = 10)\n)\ntest_linear_regression_data_not_homoscedastic$y <-\n  2 * test_linear_regression_data_not_homoscedastic$x1 + 0.2 *\n    test_linear_regression_data_not_homoscedastic$x2 + error\n\nplot(error,\n  ylab = \"error\", main = \"Residuals are not homoscedastic\", ylim = c(-20, 20)\n)\n```\n\nUsing our implementation of lm to fit the model.\n\n```{r}\nour_implementation_not_homoscedastic <- linear_regression(\n  test_linear_regression_data_not_homoscedastic,\n  test_linear_regression_data_not_homoscedastic$x1,\n  test_linear_regression_data_not_homoscedastic$x2,\n  y = test_linear_regression_data_not_homoscedastic$y\n)[[2]]\nour_implementation_not_homoscedastic$Residual[1] # a higher residual here\n```\n\nWe note that linear regression is not performing as well in this case.\n\n## Comparing residuals when all assumptions were met versus not\n\n```{r}\nresidual_comparison <-\n  t(\n    data.frame(\n      resid_all_assumptions_met = our_implementation$Residual[1],\n      resid_not_linear = our_implementation_not_linear$Residual[1],\n      resid_not_normally_dist = our_implementation_not_normally_dist$Residual[1],\n      resid_not_homoscedastic = our_implementation_not_homoscedastic$Residual[1]\n    )\n  )\nrow.names(residual_comparison) <- c(\n  \"All assumptions met\",\n  \"Linearity assumption violated\",\n  \"Normality assumption violated\",\n  \"Homoscedasticity assumption violated\"\n)\ncolnames(residual_comparison) <- \"Residuals\"\nresidual_comparison\n```\n\n## Conclusion\n\nThe implementation of Linear Regression where all assumptions are met performs the best; i.e. it gives us predictions which are closest to the true outcome values. From the residual comparison, we also note that applying linear regression to data that aren't linear can be especially worrisome.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"linear.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title":"Linear"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}