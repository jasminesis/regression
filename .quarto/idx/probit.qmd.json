{"title":"Probit","markdown":{"yaml":{"title":"Probit"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r, message=F, echo=F}\nlibrary(knitr)\nopts_chunk$set(fig.width = 5, fig.height = 4, fig.align = \"left\", out.width = \"8.5in\")\n\nset.seed(123)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(alr4)\nlibrary(pscl)\nlibrary(glmbb) # for crabs data\nlibrary(kableExtra)\nlibrary(lmtest)\n```\n\n\nThe Probit model classifies observations into one of two categories (for simple Probit Regression; multinomial Probit Regression can classify observations into more than two categories) by estimating the probability that an observation with particular characteristics is more likely to fall in one category or another.\n\n## Uses\n\nProbit Regression is primarily used when the outcome is binary - thus, it is mainly used for classification problems. When covariates are continuous, there are infinite possible values for the outcome if using Linear Regression; Logistic and Probit Regressions are therefore better than Linear if we need to bound the outcome to 0 and 1.\n\nLogistic Regression and Probit Regressions give almost identical results - they just have different link functions. The decision to chose one over the other is discipline-dependent, and it is said that Logistic Regression is better when one has extreme independent variables (where one particular small or large value will overwhelmingly determine if your outcome is 0 or 1 - overriding the effect of most other variables). However, there is no 'right' answer to this debate.\n\n## Assumptions\n\n-   The outcome is binary\n-   The z-score of the outcome and the predictor variables have a linear relationship\n-   The errors are normally distributed and are independent of one another\n\n## Our Probit Regression Implementation\n\nOur Probit Regression implementation: (Note that we use bootstrapping to estimate standard errors)\n\n```{r}\nprobit_regression <- function(data, ..., y) {\n  n <- nrow(data)\n  x_parameters <- c(...)\n  # defining the predictor matrix\n  X <-\n    matrix(c(rep(1, n), x_parameters),\n      nrow = n,\n      ncol = ncol(data)\n    )\n  # defining the outcome matrix\n  Y <- matrix(y, nrow = n, ncol = 1)\n  # defining the log likelihood\n  probit.loglikelihood <- function(beta, X, Y) {\n    eta <- X %*% beta\n    p <- pnorm(eta)\n    loglikelihood <- -sum((1 - Y) * log(1 - p) + Y * log(p))\n    return(loglikelihood)\n  }\n  # starting with an initial guess of the parameter values\n  initial_guess <- matrix(0, nrow = ncol(data), ncol = 1)\n  # using 'optim' to maximize the log likelihood\n  result <- optim(\n    initial_guess,\n    fn = probit.loglikelihood,\n    X = X,\n    Y = Y,\n    method = NULL\n  )$par\n  # creating a vector 'estimate' for the beta coefficients\n  estimate <- result\n  # bootstrapping to estimate the standard errors\n  num_bootstraps <- 10\n  result_bootstrap <-\n    matrix(0, nrow = num_bootstraps, ncol = ncol(X))\n  for (i in 1:num_bootstraps) {\n    sample_indices <- sample(nrow(data), replace = TRUE)\n    bootstrap_data <- data[sample_indices, ]\n    X_bootstrap <-\n      matrix(\n        c(rep(1, nrow(bootstrap_data)), x_parameters),\n        nrow = nrow(bootstrap_data),\n        ncol = ncol(bootstrap_data)\n      )\n    Y_bootstrap <-\n      matrix(bootstrap_data$y,\n        nrow = nrow(bootstrap_data),\n        ncol = 1\n      )\n    initial_guess_bootstrap <-\n      matrix(0, nrow = ncol(bootstrap_data), ncol = 1)\n    result_bootstrap[i, ] <- optim(\n      initial_guess_bootstrap,\n      probit.loglikelihood,\n      X = X_bootstrap,\n      Y = Y_bootstrap,\n      method = NULL\n    )$par\n  }\n  # finding the standard deviation of the bootstrapped betas to find the\n  # standard error of the coefficients\n  se <- apply(result_bootstrap, 2, sd)\n  # calculating the z-statistic\n  z <- estimate / se\n  # defining the degrees of freedom\n  df <- nrow(X) - ncol(X)\n  # calculating the p-value\n  p <- 2 * pnorm(z, lower.tail = FALSE)\n  # defining the row names of the output data frame\n  rownames <- c()\n  for (i in 1:((ncol(X)) - 1)) {\n    rownames[i] <- i\n  }\n  data_to_plot <- data[, -which(colnames(data) == \"y\")]\n  data_to_plot$y_zscore <- qnorm(pnorm(data$y))\n  test <- list(\n    pairs(data_to_plot, main = \"Assessing Linearity of Predictors\\n with z score of Outcome\")\n  )\n  impl <- data.frame(\n    Estimate = estimate,\n    Std.Error = se,\n    z.value = z,\n    p.value = p,\n    DegOfFreedom = c(df, rep(NA, ncol(X) - 1)),\n    row.names = c(\"(Intercept)\", paste0(rep(\"x\", ncol(\n      X\n    ) - 1), rownames))\n  )\n  # returning a data frame akin to the glm probit output\n  return(list(test, impl))\n}\n```\n\nCreating a function to predict the outcomes based on our Probit Regression implementation.\n\n```{r}\npredict_probit <-\n  function(data, ..., y, implementation_probit) {\n    n <-\n      implementation_probit$DegOfFreedom[1] + nrow(implementation_probit)\n    input_covariate_values <- c(...)\n    X <-\n      matrix(\n        c(rep(1, n), input_covariate_values),\n        nrow = n,\n        ncol = nrow(implementation_probit)\n      )\n    Y <- matrix(y, nrow = n, ncol = 1)\n    estimate <-\n      implementation_probit[1:nrow(implementation_probit), 1]\n    pred <- ifelse(X %*% estimate < 0, 0, 1)\n    return(pred)\n  }\n```\n\nCreating a test data set which meets all Probit Regression assumptions to check if our function works.\n\n```{r}\ntest_probit_regression_data <- data.frame(\n  x1 = rnorm(1000, 0, 1),\n  x2 = rnorm(1000, 0, 1)\n)\nerror <- rnorm(1000, mean = 0, sd = 0.5)\ntest_probit_regression_data$y <- test_probit_regression_data$x1 +\n  0.5 * test_probit_regression_data$x2 +\n  error\ntest_probit_regression_data$y <-\n  qnorm(pnorm(test_probit_regression_data$y))\n\nplot(test_probit_regression_data$x1, test_probit_regression_data$y,\n  main = \"The z score of y and x1 have a linear relationship\", cex.main = 0.6,\n  xlab = \"x1\", ylab = \"y\"\n)\nplot(test_probit_regression_data$x1, test_probit_regression_data$y,\n  main = \"The z score of y and x2 have a linear relationship\", cex.main = 0.6,\n  xlab = \"x2\", ylab = \"y\"\n)\n\ntest_probit_regression_data$y <-\n  ifelse(test_probit_regression_data$y < 0, 0, 1)\n\nplot(density(error), main = \"Errors are normally distributed\")\n```\n\n## Testing Assumptions for Probit Regression\n\n```{r}\ntest_probit_reg <- probit_regression(test_probit_regression_data,\n  test_probit_regression_data$x1,\n  test_probit_regression_data$x2,\n  y = test_probit_regression_data$y\n)[[1]]\n```\n\nApplying the function we created on this data set.\n\n```{r}\nour_implementation_probit <-\n  probit_regression(\n    test_probit_regression_data,\n    test_probit_regression_data$x1,\n    test_probit_regression_data$x2,\n    y = test_probit_regression_data$y\n  )[[2]]\nour_implementation_probit\n```\n\nComparing our output to R's output.\n\n```{r}\nr_implementation_probit <-\n  summary(glm(y ~ x1 + x2,\n    data = test_probit_regression_data,\n    family = binomial(link = \"probit\")\n  ))\nr_implementation_probit\n```\n\nWe note that the results are similar.\n\nWe followed all assumptions of Probit Regression in regressing y on x1 and x2 using the test_probit_regression_data data set. We will compare the residual of this regression to that of all the others where assumptions will be broken.\n\nThe accuracy for where all assumptions are met:\n\n```{r}\nprediction_all_assumptions_met <-\n  as.numeric(\n    predict_probit(\n      test_probit_regression_data,\n      test_probit_regression_data$x1,\n      test_probit_regression_data$x2,\n      y = test_probit_regression_data$y,\n      implementation_probit = our_implementation_probit\n    )\n  )\naccuracy_all_assumptions_met <-\n  sum(prediction_all_assumptions_met == test_probit_regression_data$y) / 1000\naccuracy_all_assumptions_met # high accuracy here\n```\n\n## Breaking Assumptions\n\n### Breaking the assumption that the relationship between the predictors and the z score of y is linear\n\nCreating a data set where, if we apply Probit Regression, this assumption will be broken.\n\n```{r}\ntest_probit_regression_data_not_linear <-\n  data.frame(\n    x1 = rnorm(1000, 0, 1),\n    x2 = rnorm(1000, 0, 1)\n  )\neror <- rnorm(1000, mean = 0, sd = 0.5)\ntest_probit_regression_data_not_linear$y <-\n  test_probit_regression_data_not_linear$x1^2 + error\ntest_probit_regression_data_not_linear$y <-\n  qnorm(pnorm(test_probit_regression_data_not_linear$y))\n\nplot(test_probit_regression_data_not_linear$x1, test_probit_regression_data_not_linear$y,\n  main = \"The z score of y and x1 \\ndo not have a linear relationship\",\n  xlab = \"x1\", ylab = \"y\"\n)\n\ntest_probit_regression_data_not_linear$y <-\n  ifelse(test_probit_regression_data_not_linear$y < 0, 0, 1)\n```\n\nUsing our implementation of glm Probit to fit the model and get an accuracy measure.\n\n```{r}\nour_implementation_probit_not_linear <-\n  probit_regression(\n    test_probit_regression_data_not_linear,\n    test_probit_regression_data_not_linear$x1,\n    test_probit_regression_data_not_linear$x2,\n    y = test_probit_regression_data_not_linear$y\n  )[[2]]\n\nprediction_not_linear <-\n  as.numeric(\n    predict_probit(\n      test_probit_regression_data_not_linear,\n      test_probit_regression_data_not_linear$x1,\n      test_probit_regression_data_not_linear$x2,\n      y = test_probit_regression_data_not_linear$y,\n      implementation_probit = our_implementation_probit_not_linear\n    )\n  )\naccuracy_not_linear <-\n  sum(prediction_not_linear == test_probit_regression_data_not_linear$y) / 1000\naccuracy_not_linear # lower accuracy here\n```\n\nWe note that Probit Regression is not performing as well in this case.\n\n### Breaking the assumption that the errors are normally distributed\n\nCreating a data set where, if we apply Probit Regression, this assumption will be broken.\n\n```{r}\ntest_probit_regression_data_not_normally_dist <-\n  data.frame(\n    x1 = rnorm(1000, 0, 1),\n    x2 = rnorm(1000, 0, 1)\n  )\nerror <- runif(1000, min = -1, max = 1)\ntest_probit_regression_data_not_normally_dist$y <-\n  test_probit_regression_data_not_normally_dist$x1 + error\ntest_probit_regression_data_not_normally_dist$y <-\n  qnorm(pnorm(test_probit_regression_data_not_normally_dist$y))\n\nplot(density(error), main = \"Errors are not normally distributed\")\n\ntest_probit_regression_data_not_normally_dist$y <-\n  ifelse(test_probit_regression_data_not_normally_dist$y < 0, 0, 1)\n```\n\nUsing our implementation of glm Probit to fit the model and get an accuracy measure.\n\n```{r}\nour_implementation_probit_not_normally_dist <-\n  probit_regression(\n    test_probit_regression_data_not_normally_dist,\n    test_probit_regression_data_not_normally_dist$x1,\n    test_probit_regression_data_not_normally_dist$x2,\n    y = test_probit_regression_data_not_normally_dist$y\n  )[[2]]\n\nprediction_not_normally_dist <-\n  as.numeric(\n    predict_probit(\n      test_probit_regression_data_not_normally_dist,\n      test_probit_regression_data_not_normally_dist$x1,\n      test_probit_regression_data_not_normally_dist$x2,\n      y = test_probit_regression_data_not_normally_dist$y,\n      implementation_probit = our_implementation_probit_not_normally_dist\n    )\n  )\naccuracy_not_normally_dist <-\n  sum(prediction_not_normally_dist == test_probit_regression_data_not_normally_dist$y) / 1000\naccuracy_not_normally_dist # lower accuracy here\n```\n\nWe note that Probit Regression is not performing as well in this case.\n\n## Comparing accuracies when all assumptions were met versus not\n\n```{r}\naccuracy_comparison <-\n  t(\n    data.frame(\n      accuracy_all_assumptions_met,\n      accuracy_not_linear,\n      accuracy_not_normally_dist\n    )\n  )\nrow.names(accuracy_comparison) <- c(\n  \"All assumptions met\",\n  \"Linearity assumption violated\",\n  \"Normality assumption violated\"\n)\ncolnames(accuracy_comparison) <- \"Accuracy\"\naccuracy_comparison\n```\n\n## Conclusion\n\nThe implementation of Probit Regression where all assumptions are met performs the best; i.e. it gives us predictions which are more accurate to the true outcome values.\n","srcMarkdownNoYaml":"\n\n```{r, message=F, echo=F}\nlibrary(knitr)\nopts_chunk$set(fig.width = 5, fig.height = 4, fig.align = \"left\", out.width = \"8.5in\")\n\nset.seed(123)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(alr4)\nlibrary(pscl)\nlibrary(glmbb) # for crabs data\nlibrary(kableExtra)\nlibrary(lmtest)\n```\n\n## Introduction\n\nThe Probit model classifies observations into one of two categories (for simple Probit Regression; multinomial Probit Regression can classify observations into more than two categories) by estimating the probability that an observation with particular characteristics is more likely to fall in one category or another.\n\n## Uses\n\nProbit Regression is primarily used when the outcome is binary - thus, it is mainly used for classification problems. When covariates are continuous, there are infinite possible values for the outcome if using Linear Regression; Logistic and Probit Regressions are therefore better than Linear if we need to bound the outcome to 0 and 1.\n\nLogistic Regression and Probit Regressions give almost identical results - they just have different link functions. The decision to chose one over the other is discipline-dependent, and it is said that Logistic Regression is better when one has extreme independent variables (where one particular small or large value will overwhelmingly determine if your outcome is 0 or 1 - overriding the effect of most other variables). However, there is no 'right' answer to this debate.\n\n## Assumptions\n\n-   The outcome is binary\n-   The z-score of the outcome and the predictor variables have a linear relationship\n-   The errors are normally distributed and are independent of one another\n\n## Our Probit Regression Implementation\n\nOur Probit Regression implementation: (Note that we use bootstrapping to estimate standard errors)\n\n```{r}\nprobit_regression <- function(data, ..., y) {\n  n <- nrow(data)\n  x_parameters <- c(...)\n  # defining the predictor matrix\n  X <-\n    matrix(c(rep(1, n), x_parameters),\n      nrow = n,\n      ncol = ncol(data)\n    )\n  # defining the outcome matrix\n  Y <- matrix(y, nrow = n, ncol = 1)\n  # defining the log likelihood\n  probit.loglikelihood <- function(beta, X, Y) {\n    eta <- X %*% beta\n    p <- pnorm(eta)\n    loglikelihood <- -sum((1 - Y) * log(1 - p) + Y * log(p))\n    return(loglikelihood)\n  }\n  # starting with an initial guess of the parameter values\n  initial_guess <- matrix(0, nrow = ncol(data), ncol = 1)\n  # using 'optim' to maximize the log likelihood\n  result <- optim(\n    initial_guess,\n    fn = probit.loglikelihood,\n    X = X,\n    Y = Y,\n    method = NULL\n  )$par\n  # creating a vector 'estimate' for the beta coefficients\n  estimate <- result\n  # bootstrapping to estimate the standard errors\n  num_bootstraps <- 10\n  result_bootstrap <-\n    matrix(0, nrow = num_bootstraps, ncol = ncol(X))\n  for (i in 1:num_bootstraps) {\n    sample_indices <- sample(nrow(data), replace = TRUE)\n    bootstrap_data <- data[sample_indices, ]\n    X_bootstrap <-\n      matrix(\n        c(rep(1, nrow(bootstrap_data)), x_parameters),\n        nrow = nrow(bootstrap_data),\n        ncol = ncol(bootstrap_data)\n      )\n    Y_bootstrap <-\n      matrix(bootstrap_data$y,\n        nrow = nrow(bootstrap_data),\n        ncol = 1\n      )\n    initial_guess_bootstrap <-\n      matrix(0, nrow = ncol(bootstrap_data), ncol = 1)\n    result_bootstrap[i, ] <- optim(\n      initial_guess_bootstrap,\n      probit.loglikelihood,\n      X = X_bootstrap,\n      Y = Y_bootstrap,\n      method = NULL\n    )$par\n  }\n  # finding the standard deviation of the bootstrapped betas to find the\n  # standard error of the coefficients\n  se <- apply(result_bootstrap, 2, sd)\n  # calculating the z-statistic\n  z <- estimate / se\n  # defining the degrees of freedom\n  df <- nrow(X) - ncol(X)\n  # calculating the p-value\n  p <- 2 * pnorm(z, lower.tail = FALSE)\n  # defining the row names of the output data frame\n  rownames <- c()\n  for (i in 1:((ncol(X)) - 1)) {\n    rownames[i] <- i\n  }\n  data_to_plot <- data[, -which(colnames(data) == \"y\")]\n  data_to_plot$y_zscore <- qnorm(pnorm(data$y))\n  test <- list(\n    pairs(data_to_plot, main = \"Assessing Linearity of Predictors\\n with z score of Outcome\")\n  )\n  impl <- data.frame(\n    Estimate = estimate,\n    Std.Error = se,\n    z.value = z,\n    p.value = p,\n    DegOfFreedom = c(df, rep(NA, ncol(X) - 1)),\n    row.names = c(\"(Intercept)\", paste0(rep(\"x\", ncol(\n      X\n    ) - 1), rownames))\n  )\n  # returning a data frame akin to the glm probit output\n  return(list(test, impl))\n}\n```\n\nCreating a function to predict the outcomes based on our Probit Regression implementation.\n\n```{r}\npredict_probit <-\n  function(data, ..., y, implementation_probit) {\n    n <-\n      implementation_probit$DegOfFreedom[1] + nrow(implementation_probit)\n    input_covariate_values <- c(...)\n    X <-\n      matrix(\n        c(rep(1, n), input_covariate_values),\n        nrow = n,\n        ncol = nrow(implementation_probit)\n      )\n    Y <- matrix(y, nrow = n, ncol = 1)\n    estimate <-\n      implementation_probit[1:nrow(implementation_probit), 1]\n    pred <- ifelse(X %*% estimate < 0, 0, 1)\n    return(pred)\n  }\n```\n\nCreating a test data set which meets all Probit Regression assumptions to check if our function works.\n\n```{r}\ntest_probit_regression_data <- data.frame(\n  x1 = rnorm(1000, 0, 1),\n  x2 = rnorm(1000, 0, 1)\n)\nerror <- rnorm(1000, mean = 0, sd = 0.5)\ntest_probit_regression_data$y <- test_probit_regression_data$x1 +\n  0.5 * test_probit_regression_data$x2 +\n  error\ntest_probit_regression_data$y <-\n  qnorm(pnorm(test_probit_regression_data$y))\n\nplot(test_probit_regression_data$x1, test_probit_regression_data$y,\n  main = \"The z score of y and x1 have a linear relationship\", cex.main = 0.6,\n  xlab = \"x1\", ylab = \"y\"\n)\nplot(test_probit_regression_data$x1, test_probit_regression_data$y,\n  main = \"The z score of y and x2 have a linear relationship\", cex.main = 0.6,\n  xlab = \"x2\", ylab = \"y\"\n)\n\ntest_probit_regression_data$y <-\n  ifelse(test_probit_regression_data$y < 0, 0, 1)\n\nplot(density(error), main = \"Errors are normally distributed\")\n```\n\n## Testing Assumptions for Probit Regression\n\n```{r}\ntest_probit_reg <- probit_regression(test_probit_regression_data,\n  test_probit_regression_data$x1,\n  test_probit_regression_data$x2,\n  y = test_probit_regression_data$y\n)[[1]]\n```\n\nApplying the function we created on this data set.\n\n```{r}\nour_implementation_probit <-\n  probit_regression(\n    test_probit_regression_data,\n    test_probit_regression_data$x1,\n    test_probit_regression_data$x2,\n    y = test_probit_regression_data$y\n  )[[2]]\nour_implementation_probit\n```\n\nComparing our output to R's output.\n\n```{r}\nr_implementation_probit <-\n  summary(glm(y ~ x1 + x2,\n    data = test_probit_regression_data,\n    family = binomial(link = \"probit\")\n  ))\nr_implementation_probit\n```\n\nWe note that the results are similar.\n\nWe followed all assumptions of Probit Regression in regressing y on x1 and x2 using the test_probit_regression_data data set. We will compare the residual of this regression to that of all the others where assumptions will be broken.\n\nThe accuracy for where all assumptions are met:\n\n```{r}\nprediction_all_assumptions_met <-\n  as.numeric(\n    predict_probit(\n      test_probit_regression_data,\n      test_probit_regression_data$x1,\n      test_probit_regression_data$x2,\n      y = test_probit_regression_data$y,\n      implementation_probit = our_implementation_probit\n    )\n  )\naccuracy_all_assumptions_met <-\n  sum(prediction_all_assumptions_met == test_probit_regression_data$y) / 1000\naccuracy_all_assumptions_met # high accuracy here\n```\n\n## Breaking Assumptions\n\n### Breaking the assumption that the relationship between the predictors and the z score of y is linear\n\nCreating a data set where, if we apply Probit Regression, this assumption will be broken.\n\n```{r}\ntest_probit_regression_data_not_linear <-\n  data.frame(\n    x1 = rnorm(1000, 0, 1),\n    x2 = rnorm(1000, 0, 1)\n  )\neror <- rnorm(1000, mean = 0, sd = 0.5)\ntest_probit_regression_data_not_linear$y <-\n  test_probit_regression_data_not_linear$x1^2 + error\ntest_probit_regression_data_not_linear$y <-\n  qnorm(pnorm(test_probit_regression_data_not_linear$y))\n\nplot(test_probit_regression_data_not_linear$x1, test_probit_regression_data_not_linear$y,\n  main = \"The z score of y and x1 \\ndo not have a linear relationship\",\n  xlab = \"x1\", ylab = \"y\"\n)\n\ntest_probit_regression_data_not_linear$y <-\n  ifelse(test_probit_regression_data_not_linear$y < 0, 0, 1)\n```\n\nUsing our implementation of glm Probit to fit the model and get an accuracy measure.\n\n```{r}\nour_implementation_probit_not_linear <-\n  probit_regression(\n    test_probit_regression_data_not_linear,\n    test_probit_regression_data_not_linear$x1,\n    test_probit_regression_data_not_linear$x2,\n    y = test_probit_regression_data_not_linear$y\n  )[[2]]\n\nprediction_not_linear <-\n  as.numeric(\n    predict_probit(\n      test_probit_regression_data_not_linear,\n      test_probit_regression_data_not_linear$x1,\n      test_probit_regression_data_not_linear$x2,\n      y = test_probit_regression_data_not_linear$y,\n      implementation_probit = our_implementation_probit_not_linear\n    )\n  )\naccuracy_not_linear <-\n  sum(prediction_not_linear == test_probit_regression_data_not_linear$y) / 1000\naccuracy_not_linear # lower accuracy here\n```\n\nWe note that Probit Regression is not performing as well in this case.\n\n### Breaking the assumption that the errors are normally distributed\n\nCreating a data set where, if we apply Probit Regression, this assumption will be broken.\n\n```{r}\ntest_probit_regression_data_not_normally_dist <-\n  data.frame(\n    x1 = rnorm(1000, 0, 1),\n    x2 = rnorm(1000, 0, 1)\n  )\nerror <- runif(1000, min = -1, max = 1)\ntest_probit_regression_data_not_normally_dist$y <-\n  test_probit_regression_data_not_normally_dist$x1 + error\ntest_probit_regression_data_not_normally_dist$y <-\n  qnorm(pnorm(test_probit_regression_data_not_normally_dist$y))\n\nplot(density(error), main = \"Errors are not normally distributed\")\n\ntest_probit_regression_data_not_normally_dist$y <-\n  ifelse(test_probit_regression_data_not_normally_dist$y < 0, 0, 1)\n```\n\nUsing our implementation of glm Probit to fit the model and get an accuracy measure.\n\n```{r}\nour_implementation_probit_not_normally_dist <-\n  probit_regression(\n    test_probit_regression_data_not_normally_dist,\n    test_probit_regression_data_not_normally_dist$x1,\n    test_probit_regression_data_not_normally_dist$x2,\n    y = test_probit_regression_data_not_normally_dist$y\n  )[[2]]\n\nprediction_not_normally_dist <-\n  as.numeric(\n    predict_probit(\n      test_probit_regression_data_not_normally_dist,\n      test_probit_regression_data_not_normally_dist$x1,\n      test_probit_regression_data_not_normally_dist$x2,\n      y = test_probit_regression_data_not_normally_dist$y,\n      implementation_probit = our_implementation_probit_not_normally_dist\n    )\n  )\naccuracy_not_normally_dist <-\n  sum(prediction_not_normally_dist == test_probit_regression_data_not_normally_dist$y) / 1000\naccuracy_not_normally_dist # lower accuracy here\n```\n\nWe note that Probit Regression is not performing as well in this case.\n\n## Comparing accuracies when all assumptions were met versus not\n\n```{r}\naccuracy_comparison <-\n  t(\n    data.frame(\n      accuracy_all_assumptions_met,\n      accuracy_not_linear,\n      accuracy_not_normally_dist\n    )\n  )\nrow.names(accuracy_comparison) <- c(\n  \"All assumptions met\",\n  \"Linearity assumption violated\",\n  \"Normality assumption violated\"\n)\ncolnames(accuracy_comparison) <- \"Accuracy\"\naccuracy_comparison\n```\n\n## Conclusion\n\nThe implementation of Probit Regression where all assumptions are met performs the best; i.e. it gives us predictions which are more accurate to the true outcome values.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"probit.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title":"Probit"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}