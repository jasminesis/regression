{"title":"Poisson","markdown":{"yaml":{"title":"Poisson"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r, message=F, echo=F}\nlibrary(knitr)\nopts_chunk$set(fig.width = 5, fig.height = 4, fig.align = \"left\", out.width = \"8.5in\")\n\nset.seed(123)\nlibrary(tidyverse)\nlibrary(glmbb) # for crabs data\nlibrary(kableExtra)\nlibrary(MASS)\n```\n\n\nPoisson regression is used for count and rate data. We use Poisson distribution to model the expected value of $Y$, which is denoted by $E(Y) = \\mu$. The identity link is the log link, so the Poisson regression model for counts is $log(\\mu) = \\alpha + \\beta x$. The Poisson distribution with parameter $\\lambda$, $Poi(\\lambda)$ has the probability mass function\n\n$$\nP(X=k) = exp(-\\lambda)\\frac{\\lambda^k}{k!}, k=0,1,2,3,...\n$$\n\n## Uses\n\nPoisson regression can be used for count data, such as number of asthmatic attacks in one year based on the number of hospital admissions and systolic blood pressure. When the predictor variables are continuous, poisson regression ensures that the outcome variable is positive, compared to a linear regression which might predict negative counts. Another use case for Poisson regression is when the number of cases is small relative to the number of no events, such as when the number of deaths due to COVID-19 are small relative to the total population size. Logistic regression is more useful when we have data on both the binary outcomes (e.g. death and non-deaths).\n\n## Assumptions\n\n-   outcome variable must be count data\n-   Independent observations\n-   Distribution of counts follow a Poisson distribution\n-   No overdispersion - the mean and variance of the distribution are equal. If the variance is greater than the mean, negative binomial regression may be more appropriate\n\n## Our Poisson Regression Implementation\n\n```{r}\npoisson_function <- function(fn_formula, data, predict = F) {\n  number_omitted <- nrow(data) - nrow(na.omit(data))\n  data <- na.omit(data)\n\n  vars <- all.vars(as.formula(fn_formula))\n  y_name <- vars[1]\n  x_name <- vars[2:length(vars)]\n  n <- nrow(data)\n  Y <- matrix(data[, y_name], nrow = n, ncol = 1)\n  X <- matrix(cbind(rep(1, n)))\n\n  # take in categorical data\n  var_names <- vector(\"character\")\n  for (i in x_name) {\n    if (suppressWarnings(all(!is.na(as.numeric(as.character(data[, i])))))) {\n      X <- cbind(X, as.numeric(as.character(data[, i])))\n      var_names <- c(var_names, i)\n    } else {\n      categories <- sort(unique(data[, i]))\n      for (j in categories[2:length(categories)]) {\n        new_col_name <- paste0(i, j)\n        new_col <- ifelse(data[, i] == j, 1, 0)\n        X <- cbind(X, new_col)\n        var_names <- c(var_names, new_col_name)\n      }\n    }\n  }\n  optim_poisson <- function(beta, X, Y) {\n    beta <- as.matrix(beta, nrow = 4)\n    beta_x <- X %*% beta\n    loglikelihood <- -sum(Y * beta_x - exp(beta_x))\n    return(loglikelihood)\n  }\n  result <- optim(par = rep(0, ncol(X)), fn = optim_poisson, X = X, Y = Y, hessian = T)\n  OI <- solve(result$hessian)\n  se <- sqrt(diag(OI))\n  z_value <- result$par / se\n  df <- nrow(X) - ncol(X)\n  p_value <- 2 * pnorm(-1 * abs(z_value))\n\n  coef <- rbind(result$par, se, z_value, p_value)\n  colnames(coef) <- c(\"(Intercept)\", var_names)\n  rownames(coef) <- c(\"Estimate\", \"Std. Error\", \"z value\", \"p value\")\n\n  b_hat <- result$par\n  predictions <- exp(X %*% b_hat)\n\n  if (predict) {\n    return(predictions)\n  } else {\n    return(t(coef))\n  }\n}\n```\n\nTesting poisson implementation with simulated data\n\n```{r}\nn <- 100\nx1 <- sample(0:1, n, replace = T)\nlambda <- exp(2 + 0.5 * x1)\ny <- rpois(n, lambda)\nsim_data <- data.frame(y, x1)\nm1 <- glm(y ~ x1, family = poisson, data = sim_data)\nsummary(m1)$coef\npoisson_function(fn_formula = \"y ~ x1\", data = sim_data)\n\nggplot(sim_data) +\n  geom_histogram(aes(x = y, fill = factor(x1))) +\n  facet_wrap(~x1)\n```\n\nShow that our implementation of poisson regression can also make predictions\n\n```{r}\nidx <- sample(1:n, 5)\np1 <- poisson_function(fn_formula = \"y ~ x1\", data = sim_data, predict = T)[idx]\np2 <- predict(m1, type = \"response\")[idx]\n\ncompare_predict_data <- data.frame(p1, p2)\ncolnames(compare_predict_data) <- c(\"Our implementation\", \"GLM\")\n\nkable(compare_predict_data, digits = 3, caption = \"Comparison of Poisson prediction\", booktabs = TRUE, valign = \"t\") |> kable_styling(latex_options = \"HOLD_position\")\n```\n\nTesting poisson implementation with `crabs` data\n\n```{r}\n# comparing coefficients with crabs data\ndata(crabs, package = \"glmbb\")\nsummary(glm(satell ~ width, family = poisson(link = \"log\"), data = crabs))$coef\n\n# a bit over-dispersed\nmean(crabs$satell)\nvar(crabs$satell)\n\npoisson_function(fn_formula = \"satell ~ width\", data = crabs)\nggplot(crabs) +\n  geom_histogram(aes(x = satell),\n    binwidth = 1,\n    fill = \"forestgreen\", color = \"gray\"\n  )\n```\n\n**Interpretation of coefficients**\n\nA change in 1 unit of width has a multiplicative effect on the mean of $Y$. For a 1 unit increase in log(width), the estimated mean number of satellites increases by a factor of $e^{0.164} = 1.178$ when the log linear model is $log(\\mu_i) = -3.3 + 0.164 * width_i$.\n\n## Breaking Assumptions\n\n#### Independent observations\n\n```{r}\nn <- 100\nx1 <- sample(0:5, n, replace = T)\nlambda <- exp(1.5 + 0.5 * x1)\ny <- rpois(n, lambda)\nsim_data <- data.frame(y, x1)\nmp <- glm(y ~ x1, data = sim_data, family = poisson)\nplot(mp, which = 1)\n\nmp <- glm(satell ~ width, data = crabs, family = poisson)\nplot(mp, which = 1)\n```\n\nCreating fake data that has good residuals vs crabs data which has a fitted line that is not at zero. It is easy to think about why the observations may not be independent for the crabs data, if a few are observed in clusters of units, or in certain ecologies.\n\n#### Distribution of counts follow a Poisson distribution\n\n```{r warning=F}\nn <- 100\nx1 <- runif(n, 0, 10)\nx2 <- runif(n, -10, 0)\nlambda <- exp(1.5 + 0.5 * x1 + 0.5 * x2)\ny <- rpois(n, lambda)\ngood_data <- data.frame(y, x1, x2)\nlog.lambda <- log(poisson_function(fn_formula = \"y ~ x1 + x2\", data = good_data, predict = T))\nplot_data <- data.frame(log.lambda, x1, x2) |> gather(key = \"predictors\", value = \"predictor_value\", -log.lambda)\n\nggplot(plot_data, aes(x = log.lambda, y = predictor_value)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  geom_smooth(method = \"loess\") +\n  theme_bw() +\n  facet_wrap(~predictors, scales = \"free_y\")\n\n# show non-linear data\nn <- 100\nx1 <- runif(n, 0, 10)\nx2 <- runif(n, -10, 0)\nlambda <- exp(0.5 * x1^2 + 0.5 * x1 * x2)\ny <- rpois(n, lambda)\nworse_data <- data.frame(y, x1, x2)\nlog.lambda <- log(poisson_function(fn_formula = \"y ~ x1 + x2\", data = worse_data, predict = T))\nplot_data <- data.frame(log.lambda, x1, x2) |> gather(key = \"predictors\", value = \"predictor_value\", -log.lambda)\n\nggplot(plot_data, aes(x = log.lambda, y = predictor_value)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  geom_smooth(method = \"loess\") +\n  theme_bw() +\n  facet_wrap(~predictors, scales = \"free_y\")\n\n# show for binary x1\nn <- 100\nx1 <- sample(0:1, n, replace = T)\nlambda <- exp(2 + 0.5 * x1)\ny <- rpois(n, lambda)\nsim_data <- data.frame(y, x1)\nlog.lambda <- log(poisson_function(fn_formula = \"y ~ x1\", data = sim_data, predict = T))\nplot_data <- data.frame(log.lambda, x1) |> gather(key = \"predictors\", value = \"predictor_value\", -log.lambda)\n\nggplot(plot_data, aes(x = log.lambda, y = predictor_value)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  geom_smooth(method = \"loess\") +\n  theme_bw() +\n  facet_wrap(~predictors, scales = \"free_y\")\n```\n\nDoes the Poisson distribution predict well? We want to see a linear relationship between $log(\\lambda)$ and each predictor variable. This will only be visible for continuous predictor data.\n\n#### No overdispersion - the mean and variance of the distribution are equal\n\nOne of the main assumptions for Poisson regression is that the mean and variance are equal. When the variance is larger than the mean, there is overdispersion. This can be formally tested using the the overdispersion parameter.\n\n```{r}\ndata(crabs, package = \"glmbb\")\nM1 <- glm(satell ~ width, family = poisson(link = \"log\"), data = crabs)\nM2 <- glm.nb(satell ~ width, data = crabs)\nM3 <- pscl::zeroinfl(satell ~ width | width, data = crabs)\n# estimate overdispersion\nestimate_overdisp <- function(model_obj, data) {\n  z <- resid(model_obj, type = \"pearson\")\n  n <- nrow(data)\n  k <- length(coef(model_obj))\n  overdisp_ratio <- sum(z^2) / (n - k)\n  p_val <- pchisq(sum(z^2), (n - k))\n  return(cat(\"overdispersion ratio: \", overdisp_ratio, \"\\n\", \"p-value:\", p_val, \"\\n\"))\n}\nestimate_overdisp(M1, crabs)\nestimate_overdisp(M2, crabs)\nestimate_overdisp(M3, crabs)\n```\n\nThe estimated overdispersion for the crabs data is 3.18, which is large, and has a p-value of 1, which indicates that the probability is essentially zero that a random variable from the distribution would be so large. When the negative binomial model is fitted, the crabs data has an estimated overdispersion of 0.85, with a smaller p-value. It is still overdispersed relative to a negative binomial distribution, but to a smaller scale than it was to a Poisson distribution.\n\n## Conclusion\n\nMeeting the assumptions for a Poisson regression, particularly for dispersion, is quite difficult, even when simulating data. It is unclear when the mean and variance of a distribution might happen to be naturally the same.\n","srcMarkdownNoYaml":"\n\n```{r, message=F, echo=F}\nlibrary(knitr)\nopts_chunk$set(fig.width = 5, fig.height = 4, fig.align = \"left\", out.width = \"8.5in\")\n\nset.seed(123)\nlibrary(tidyverse)\nlibrary(glmbb) # for crabs data\nlibrary(kableExtra)\nlibrary(MASS)\n```\n\n## Introduction\n\nPoisson regression is used for count and rate data. We use Poisson distribution to model the expected value of $Y$, which is denoted by $E(Y) = \\mu$. The identity link is the log link, so the Poisson regression model for counts is $log(\\mu) = \\alpha + \\beta x$. The Poisson distribution with parameter $\\lambda$, $Poi(\\lambda)$ has the probability mass function\n\n$$\nP(X=k) = exp(-\\lambda)\\frac{\\lambda^k}{k!}, k=0,1,2,3,...\n$$\n\n## Uses\n\nPoisson regression can be used for count data, such as number of asthmatic attacks in one year based on the number of hospital admissions and systolic blood pressure. When the predictor variables are continuous, poisson regression ensures that the outcome variable is positive, compared to a linear regression which might predict negative counts. Another use case for Poisson regression is when the number of cases is small relative to the number of no events, such as when the number of deaths due to COVID-19 are small relative to the total population size. Logistic regression is more useful when we have data on both the binary outcomes (e.g. death and non-deaths).\n\n## Assumptions\n\n-   outcome variable must be count data\n-   Independent observations\n-   Distribution of counts follow a Poisson distribution\n-   No overdispersion - the mean and variance of the distribution are equal. If the variance is greater than the mean, negative binomial regression may be more appropriate\n\n## Our Poisson Regression Implementation\n\n```{r}\npoisson_function <- function(fn_formula, data, predict = F) {\n  number_omitted <- nrow(data) - nrow(na.omit(data))\n  data <- na.omit(data)\n\n  vars <- all.vars(as.formula(fn_formula))\n  y_name <- vars[1]\n  x_name <- vars[2:length(vars)]\n  n <- nrow(data)\n  Y <- matrix(data[, y_name], nrow = n, ncol = 1)\n  X <- matrix(cbind(rep(1, n)))\n\n  # take in categorical data\n  var_names <- vector(\"character\")\n  for (i in x_name) {\n    if (suppressWarnings(all(!is.na(as.numeric(as.character(data[, i])))))) {\n      X <- cbind(X, as.numeric(as.character(data[, i])))\n      var_names <- c(var_names, i)\n    } else {\n      categories <- sort(unique(data[, i]))\n      for (j in categories[2:length(categories)]) {\n        new_col_name <- paste0(i, j)\n        new_col <- ifelse(data[, i] == j, 1, 0)\n        X <- cbind(X, new_col)\n        var_names <- c(var_names, new_col_name)\n      }\n    }\n  }\n  optim_poisson <- function(beta, X, Y) {\n    beta <- as.matrix(beta, nrow = 4)\n    beta_x <- X %*% beta\n    loglikelihood <- -sum(Y * beta_x - exp(beta_x))\n    return(loglikelihood)\n  }\n  result <- optim(par = rep(0, ncol(X)), fn = optim_poisson, X = X, Y = Y, hessian = T)\n  OI <- solve(result$hessian)\n  se <- sqrt(diag(OI))\n  z_value <- result$par / se\n  df <- nrow(X) - ncol(X)\n  p_value <- 2 * pnorm(-1 * abs(z_value))\n\n  coef <- rbind(result$par, se, z_value, p_value)\n  colnames(coef) <- c(\"(Intercept)\", var_names)\n  rownames(coef) <- c(\"Estimate\", \"Std. Error\", \"z value\", \"p value\")\n\n  b_hat <- result$par\n  predictions <- exp(X %*% b_hat)\n\n  if (predict) {\n    return(predictions)\n  } else {\n    return(t(coef))\n  }\n}\n```\n\nTesting poisson implementation with simulated data\n\n```{r}\nn <- 100\nx1 <- sample(0:1, n, replace = T)\nlambda <- exp(2 + 0.5 * x1)\ny <- rpois(n, lambda)\nsim_data <- data.frame(y, x1)\nm1 <- glm(y ~ x1, family = poisson, data = sim_data)\nsummary(m1)$coef\npoisson_function(fn_formula = \"y ~ x1\", data = sim_data)\n\nggplot(sim_data) +\n  geom_histogram(aes(x = y, fill = factor(x1))) +\n  facet_wrap(~x1)\n```\n\nShow that our implementation of poisson regression can also make predictions\n\n```{r}\nidx <- sample(1:n, 5)\np1 <- poisson_function(fn_formula = \"y ~ x1\", data = sim_data, predict = T)[idx]\np2 <- predict(m1, type = \"response\")[idx]\n\ncompare_predict_data <- data.frame(p1, p2)\ncolnames(compare_predict_data) <- c(\"Our implementation\", \"GLM\")\n\nkable(compare_predict_data, digits = 3, caption = \"Comparison of Poisson prediction\", booktabs = TRUE, valign = \"t\") |> kable_styling(latex_options = \"HOLD_position\")\n```\n\nTesting poisson implementation with `crabs` data\n\n```{r}\n# comparing coefficients with crabs data\ndata(crabs, package = \"glmbb\")\nsummary(glm(satell ~ width, family = poisson(link = \"log\"), data = crabs))$coef\n\n# a bit over-dispersed\nmean(crabs$satell)\nvar(crabs$satell)\n\npoisson_function(fn_formula = \"satell ~ width\", data = crabs)\nggplot(crabs) +\n  geom_histogram(aes(x = satell),\n    binwidth = 1,\n    fill = \"forestgreen\", color = \"gray\"\n  )\n```\n\n**Interpretation of coefficients**\n\nA change in 1 unit of width has a multiplicative effect on the mean of $Y$. For a 1 unit increase in log(width), the estimated mean number of satellites increases by a factor of $e^{0.164} = 1.178$ when the log linear model is $log(\\mu_i) = -3.3 + 0.164 * width_i$.\n\n## Breaking Assumptions\n\n#### Independent observations\n\n```{r}\nn <- 100\nx1 <- sample(0:5, n, replace = T)\nlambda <- exp(1.5 + 0.5 * x1)\ny <- rpois(n, lambda)\nsim_data <- data.frame(y, x1)\nmp <- glm(y ~ x1, data = sim_data, family = poisson)\nplot(mp, which = 1)\n\nmp <- glm(satell ~ width, data = crabs, family = poisson)\nplot(mp, which = 1)\n```\n\nCreating fake data that has good residuals vs crabs data which has a fitted line that is not at zero. It is easy to think about why the observations may not be independent for the crabs data, if a few are observed in clusters of units, or in certain ecologies.\n\n#### Distribution of counts follow a Poisson distribution\n\n```{r warning=F}\nn <- 100\nx1 <- runif(n, 0, 10)\nx2 <- runif(n, -10, 0)\nlambda <- exp(1.5 + 0.5 * x1 + 0.5 * x2)\ny <- rpois(n, lambda)\ngood_data <- data.frame(y, x1, x2)\nlog.lambda <- log(poisson_function(fn_formula = \"y ~ x1 + x2\", data = good_data, predict = T))\nplot_data <- data.frame(log.lambda, x1, x2) |> gather(key = \"predictors\", value = \"predictor_value\", -log.lambda)\n\nggplot(plot_data, aes(x = log.lambda, y = predictor_value)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  geom_smooth(method = \"loess\") +\n  theme_bw() +\n  facet_wrap(~predictors, scales = \"free_y\")\n\n# show non-linear data\nn <- 100\nx1 <- runif(n, 0, 10)\nx2 <- runif(n, -10, 0)\nlambda <- exp(0.5 * x1^2 + 0.5 * x1 * x2)\ny <- rpois(n, lambda)\nworse_data <- data.frame(y, x1, x2)\nlog.lambda <- log(poisson_function(fn_formula = \"y ~ x1 + x2\", data = worse_data, predict = T))\nplot_data <- data.frame(log.lambda, x1, x2) |> gather(key = \"predictors\", value = \"predictor_value\", -log.lambda)\n\nggplot(plot_data, aes(x = log.lambda, y = predictor_value)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  geom_smooth(method = \"loess\") +\n  theme_bw() +\n  facet_wrap(~predictors, scales = \"free_y\")\n\n# show for binary x1\nn <- 100\nx1 <- sample(0:1, n, replace = T)\nlambda <- exp(2 + 0.5 * x1)\ny <- rpois(n, lambda)\nsim_data <- data.frame(y, x1)\nlog.lambda <- log(poisson_function(fn_formula = \"y ~ x1\", data = sim_data, predict = T))\nplot_data <- data.frame(log.lambda, x1) |> gather(key = \"predictors\", value = \"predictor_value\", -log.lambda)\n\nggplot(plot_data, aes(x = log.lambda, y = predictor_value)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  geom_smooth(method = \"loess\") +\n  theme_bw() +\n  facet_wrap(~predictors, scales = \"free_y\")\n```\n\nDoes the Poisson distribution predict well? We want to see a linear relationship between $log(\\lambda)$ and each predictor variable. This will only be visible for continuous predictor data.\n\n#### No overdispersion - the mean and variance of the distribution are equal\n\nOne of the main assumptions for Poisson regression is that the mean and variance are equal. When the variance is larger than the mean, there is overdispersion. This can be formally tested using the the overdispersion parameter.\n\n```{r}\ndata(crabs, package = \"glmbb\")\nM1 <- glm(satell ~ width, family = poisson(link = \"log\"), data = crabs)\nM2 <- glm.nb(satell ~ width, data = crabs)\nM3 <- pscl::zeroinfl(satell ~ width | width, data = crabs)\n# estimate overdispersion\nestimate_overdisp <- function(model_obj, data) {\n  z <- resid(model_obj, type = \"pearson\")\n  n <- nrow(data)\n  k <- length(coef(model_obj))\n  overdisp_ratio <- sum(z^2) / (n - k)\n  p_val <- pchisq(sum(z^2), (n - k))\n  return(cat(\"overdispersion ratio: \", overdisp_ratio, \"\\n\", \"p-value:\", p_val, \"\\n\"))\n}\nestimate_overdisp(M1, crabs)\nestimate_overdisp(M2, crabs)\nestimate_overdisp(M3, crabs)\n```\n\nThe estimated overdispersion for the crabs data is 3.18, which is large, and has a p-value of 1, which indicates that the probability is essentially zero that a random variable from the distribution would be so large. When the negative binomial model is fitted, the crabs data has an estimated overdispersion of 0.85, with a smaller p-value. It is still overdispersed relative to a negative binomial distribution, but to a smaller scale than it was to a Poisson distribution.\n\n## Conclusion\n\nMeeting the assumptions for a Poisson regression, particularly for dispersion, is quite difficult, even when simulating data. It is unclear when the mean and variance of a distribution might happen to be naturally the same.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"poisson.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title":"Poisson"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}