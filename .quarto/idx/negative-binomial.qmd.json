{"title":"Negative Binomial","markdown":{"yaml":{"title":"Negative Binomial"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r, message=F, echo=F}\nlibrary(knitr)\nopts_chunk$set(fig.width = 5, fig.height = 4, fig.align = \"left\", out.width = \"8.5in\")\n\nset.seed(123)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(alr4)\nlibrary(pscl)\nlibrary(glmbb) # for crabs data\nlibrary(kableExtra)\nlibrary(lmtest)\n```\n\n\nNegative Binomial Regression is used for predicting count data, similar to Poisson Regression, but the Negative Binomial is more flexible as it allows for the variance of the outcome to be greater than its mean (in Poisson Regression, they are assumed to be equal).\n\n## Uses\n\nNegative Binomial Regression is used to model count data with excess zeros (as in the Zero-Inflated Negative Binomial Regression) and is used to model rare events which are less likely to have counts where mean = variance. Negative Binomial can be extended to handle correlated/clustered data as well.\n\n## Assumptions\n\n-   The outcome represents count data\n-   The variance of the outcome is greater than its mean\n-   The relationship between the predictors and the log of the outcome's mean is linear\n-   The errors are independent of one another\n\n## Our Negative Binomial Regression Implementation\n\nOur Negative Binomial Regression implementation: (Note that we use bootstrapping to estimate standard errors)\n\n```{r}\nnegative_binomial_regression <- function(data, ..., y) {\n  n <- nrow(data)\n  x_parameters <- c(...)\n  # defining the predictor matrix\n  X <-\n    matrix(c(rep(1, n), x_parameters),\n      nrow = n,\n      ncol = ncol(data)\n    )\n  # defining the outcome matrix\n  Y <- matrix(y, nrow = n, ncol = 1)\n  # starting with theta = 1\n  theta <- 1\n  # defining the log likelihood\n  negative_binomial.likelihood <- function(beta, X, Y = y) {\n    eta <- X %*% beta\n    mu <- exp(eta)\n    loglikelihood <-\n      sum(Y * log(mu) - (Y + 1 / theta) * log(1 + mu / theta))\n    return(loglikelihood)\n  }\n  # starting with an initial guess of the parameter values\n  initial_guess <- rep(0, ncol(X))\n  # using 'optim' to maximize the log likelihood\n  result <- optim(\n    initial_guess,\n    negative_binomial.likelihood,\n    X = X,\n    Y = Y,\n    control = list(fnscale = -1),\n    hessian = T,\n    method = NULL\n  )$par\n  # creating a vector 'estimate' for the beta coefficients\n  estimate <- result\n  # bootstrapping to estimate the standard errors\n  num_bootstraps <- 10\n  result_bootstrap <-\n    matrix(0, nrow = num_bootstraps, ncol = ncol(X))\n  for (i in 1:num_bootstraps) {\n    sample_indices <- sample(nrow(data), replace = TRUE)\n    bootstrap_data <- data[sample_indices, ]\n    X_bootstrap <-\n      matrix(\n        c(rep(1, nrow(bootstrap_data)), x_parameters),\n        nrow = nrow(bootstrap_data),\n        ncol = ncol(bootstrap_data)\n      )\n    Y_bootstrap <-\n      matrix(bootstrap_data$y,\n        nrow = nrow(bootstrap_data),\n        ncol = 1\n      )\n    initial_guess_bootstrap <-\n      matrix(0, nrow = ncol(bootstrap_data), ncol = 1)\n    result_bootstrap[i, ] <- optim(\n      initial_guess_bootstrap,\n      negative_binomial.likelihood,\n      X = X_bootstrap,\n      Y = Y_bootstrap,\n      control = list(fnscale = -1),\n      hessian = T,\n      method = NULL\n    )$par\n  }\n  # finding the standard deviation of the bootstrapped betas to find the\n  # standard error of the coefficients\n  se <- apply(result_bootstrap, 2, sd)\n  # calculating the z-statistic\n  z <- estimate / se\n  # defining the degrees of freedom\n  df <- nrow(X) - ncol(X)\n  # calculating the p-value\n  p <- 2 * pnorm(z, lower.tail = FALSE)\n  # defining the row names of the output data frame\n  rownames <- c()\n  for (i in 1:((ncol(X)) - 1)) {\n    rownames[i] <- i\n  }\n  data_to_plot <- data[, -which(colnames(data) == \"y\")]\n  data_to_plot$y_log <- log(data$y)\n  test <- list(\n    pairs(data_to_plot, main = \"Assessing Linearity of Predictors \\nwith log of Outcome\")\n  )\n  impl <- data.frame(\n    Estimate = estimate,\n    Std.Error = se,\n    z.value = z,\n    p.value = p,\n    DegOfFreedom = c(df, rep(NA, ncol(X) - 1)),\n    row.names = c(\"(Intercept)\", paste0(rep(\"x\", ncol(\n      X\n    ) - 1), rownames))\n  )\n  # returning a data frame akin to the glm probit output\n  return(list(test, impl))\n}\n```\n\nCreating a function to predict the outcomes based on our Negative Binomial Regression implementation.\n\n```{r}\npredict_neg_binom <-\n  function(data, ..., y, implementation_neg_binom) {\n    n <-\n      implementation_neg_binom$DegOfFreedom[1] + nrow(implementation_neg_binom)\n    input_covariate_values <- c(...)\n    X <-\n      matrix(\n        c(rep(1, n), input_covariate_values),\n        nrow = n,\n        ncol = nrow(implementation_neg_binom)\n      )\n    Y <- matrix(y, nrow = n, ncol = 1)\n    estimate <-\n      implementation_neg_binom[1:nrow(implementation_neg_binom), 1]\n    pred <- exp(X %*% estimate)\n    return(pred)\n  }\n```\n\nCreating a test data set which meets all Negative Binomial Regression assumptions to check if our function works.\n\n```{r}\nx1 <- rnorm(100, mean = 0, sd = 0.5)\nx2 <- rnorm(100, mean = 0, sd = 0.5)\ny <- rnbinom(100, mu = exp(x1 + x2), size = 0.5)\ntest_neg_binom_regression_data <- data.frame(x1, x2, y)\n# to ensure that the variance of the outcome variable is greater\n# than its mean\nvar(y) > mean(y)\nplot(test_neg_binom_regression_data$x1, log(test_neg_binom_regression_data$y),\n  main = \"The relationship between the log of the outcome and x1 is linear (it is not apparent in this plot but our data structure captures this relationship)\", cex.main = 0.4,\n  xlab = \"x1\", ylab = \"y\"\n)\nplot(test_neg_binom_regression_data$x2, log(test_neg_binom_regression_data$y),\n  xlab = \"x2\", ylab = \"y\",\n  main = \"The relationship between the log of the outcome and x2 is linear (it is not apparent in this plot but our data structure captures this relationship)\", cex.main = 0.4\n)\n```\n\n## Testing Assumptions for Negative Binomial Regression\n\n```{r}\ntest_negbinom_reg <- negative_binomial_regression(test_neg_binom_regression_data,\n  test_neg_binom_regression_data$x1,\n  test_neg_binom_regression_data$x2,\n  y = test_neg_binom_regression_data$y\n)[[1]]\n```\n\nUsing our implementation of Negative Binomial to fit the model and get residual measure.\n\n```{r}\nour_implementation_neg_binom <-\n  negative_binomial_regression(\n    test_neg_binom_regression_data,\n    test_neg_binom_regression_data$x1,\n    test_neg_binom_regression_data$x2,\n    y = test_neg_binom_regression_data$y\n  )[[2]]\nour_implementation_neg_binom\n```\n\nComparing our output to R's output.\n\n```{r}\nr_implementation_neg_binom <-\n  summary(glm.nb(y ~ x1 + x2, data = test_neg_binom_regression_data))\nr_implementation_neg_binom\n```\n\nWe note that the results are similar.\n\nWe followed all assumptions of Negative Binomial Regression in regressing y on x1 and x2 using the test_neg_binom_regression_data data set. We will compare the residual of this regression to that of all the others where assumptions will be broken.\n\nThe residual for where all assumptions are met:\n\n```{r}\nprediction_all_assumptions_met <-\n  as.numeric(\n    predict_neg_binom(\n      test_neg_binom_regression_data,\n      test_neg_binom_regression_data$x1,\n      test_neg_binom_regression_data$x2,\n      y = test_neg_binom_regression_data$y,\n      implementation_neg_binom = our_implementation_neg_binom\n    )\n  )\nresidual_all_assumptions_met <- sqrt(mean((\n  test_neg_binom_regression_data$y - prediction_all_assumptions_met\n)^2))\nresidual_all_assumptions_met # small residual\n# residual plot\nplot(\n  test_neg_binom_regression_data$y - prediction_all_assumptions_met,\n  ylim = c(-30, 30),\n  ylab = \"Residuals\",\n  main = \"Residual Plot: All assumptions met\",\n  pch = 16\n)\nabline(\n  h = 0,\n  col = \"red\",\n  lty = 2,\n  lwd = 3\n)\n```\n\n## Breaking Assumptions\n\n### Breaking the assumption that the relationship between the predictors and the log of the outcome's mean is linear\n\nCreating a data set where, if we apply Negative Binomial regression, this assumption will be broken.\n\n```{r}\nx1 <- rnorm(100, mean = 0, sd = 0.5)\nx2 <- rnorm(100, mean = 0, sd = 0.5)\ny <- rnbinom(100, mu = exp(x1 + x2)^2, size = 0.5)\ntest_neg_binom_regression_data_not_linear <- data.frame(x1, x2, y)\n# to ensure that the variance of the outcome variable is greater\n# than its mean\nvar(y) > mean(y)\nplot(test_neg_binom_regression_data_not_linear$x1, log(test_neg_binom_regression_data_not_linear$y),\n  main = \"The relationship between the log of the outcome and x1 is not linear\", cex.main = 0.8,\n  xlab = \"x1\", ylab = \"y\"\n)\nplot(test_neg_binom_regression_data_not_linear$x2, log(test_neg_binom_regression_data_not_linear$y),\n  xlab = \"x2\", ylab = \"y\", cex.main = 0.8,\n  main = \"The relationship between the log of the outcome and x2 is not linear\"\n)\n```\n\nUsing our implementation of Negative Binomial to fit the model and get a residual measure.\n\n```{r}\nour_implementation_neg_binom_not_linear <-\n  negative_binomial_regression(\n    test_neg_binom_regression_data_not_linear,\n    test_neg_binom_regression_data_not_linear$x1,\n    test_neg_binom_regression_data_not_linear$x2,\n    y = test_neg_binom_regression_data_not_linear$y\n  )[[2]]\n\nprediction_not_linear <-\n  as.numeric(\n    predict_neg_binom(\n      test_neg_binom_regression_data_not_linear,\n      test_neg_binom_regression_data_not_linear$x1,\n      test_neg_binom_regression_data_not_linear$x2,\n      y = test_neg_binom_regression_data_not_linear$y,\n      implementation_neg_binom = our_implementation_neg_binom_not_linear\n    )\n  )\nresidual_not_linear <- sqrt(mean((\n  test_neg_binom_regression_data_not_linear$y - prediction_not_linear\n)^2))\nresidual_not_linear # large residual\n# residual plot\nplot(\n  test_neg_binom_regression_data_not_linear$y - prediction_not_linear,\n  ylim = c(-30, 30),\n  ylab = \"Residuals\",\n  main = \"Residual Plot: Linearity assumption violated\",\n  pch = 16\n)\nabline(\n  h = 0,\n  col = \"red\",\n  lty = 2,\n  lwd = 3\n)\n```\n\n### Breaking the assumption that the mean of the outcome is smaller than its variance\n\nCreating a data set where, if we apply Negative Binomial regression, this assumption will be broken.\n\n```{r}\nx1 <- rnorm(100, mean = 0, sd = 0.5)\nx2 <- rnorm(100, mean = 0, sd = 0.5)\ny <- rnbinom(100, mu = exp(x2 - 2 * x1), size = 100) + 10\ntest_neg_binom_regression_data_mean_greater <- data.frame(x1, x2, y)\n# to ensure that the variance of the outcome variable is smaller\n# than its mean\nvar(y) > mean(y)\n```\n\nUsing our implementation of Negative Binomial to fit the model and get a residual measure.\n\n```{r}\nour_implementation_neg_binom_mean_greater <-\n  negative_binomial_regression(\n    test_neg_binom_regression_data_mean_greater,\n    test_neg_binom_regression_data_mean_greater$x1,\n    test_neg_binom_regression_data_mean_greater$x2,\n    y = test_neg_binom_regression_data_mean_greater$y\n  )[[2]]\n\nprediction_mean_greater <-\n  as.numeric(\n    predict_neg_binom(\n      test_neg_binom_regression_data_mean_greater,\n      test_neg_binom_regression_data_mean_greater$x1,\n      test_neg_binom_regression_data_mean_greater$x2,\n      y = test_neg_binom_regression_data_mean_greater$y,\n      implementation_neg_binom = our_implementation_neg_binom_mean_greater\n    )\n  )\nresidual_mean_greater <- sqrt(mean((\n  test_neg_binom_regression_data_mean_greater$y - prediction_mean_greater\n)^2))\nresidual_mean_greater\n# residual plot\nplot(\n  test_neg_binom_regression_data_mean_greater$y - prediction_mean_greater,\n  ylim = c(-30, 30),\n  ylab = \"Residuals\",\n  cex.main = 0.9,\n  main = \"Residual Plot: Variance of outcome greater than mean assumption violated\",\n  pch = 16\n)\nabline(\n  h = 0,\n  col = \"red\",\n  lty = 2,\n  lwd = 3\n)\n```\n\n## Comparing residuals when all assumptions were met versus not\n\n```{r}\nresidual_comparison <-\n  t(\n    data.frame(\n      residual_all_assumptions_met,\n      residual_not_linear,\n      residual_mean_greater\n    )\n  )\nrow.names(residual_comparison) <- c(\n  \"All assumptions met\",\n  \"Linearity assumption violated\",\n  \"Variance > Mean assumption violated\"\n)\ncolnames(residual_comparison) <- \"Residuals\"\nresidual_comparison\n```\n\n## Conclusion\n\nThe implementation of Negative Binomial Regression where all assumptions are met performs well; however, even the model where an assumption is broken; i.e. where the mean of the outcome is greater than its variance, performs well too - however, it should be noted that even though its predictions might be accurate, its standard errors and p-values might be biased.\n\n```{r, message=F}\nset.seed(123)\nlibrary(alr4)\nlibrary(tidyverse)\nlibrary(MASS)\nlibrary(pscl)\nlibrary(glmbb) # for crabs data\nlibrary(kableExtra)\nlibrary(lmtest)\n```\n","srcMarkdownNoYaml":"\n\n```{r, message=F, echo=F}\nlibrary(knitr)\nopts_chunk$set(fig.width = 5, fig.height = 4, fig.align = \"left\", out.width = \"8.5in\")\n\nset.seed(123)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(alr4)\nlibrary(pscl)\nlibrary(glmbb) # for crabs data\nlibrary(kableExtra)\nlibrary(lmtest)\n```\n\n## Introduction\n\nNegative Binomial Regression is used for predicting count data, similar to Poisson Regression, but the Negative Binomial is more flexible as it allows for the variance of the outcome to be greater than its mean (in Poisson Regression, they are assumed to be equal).\n\n## Uses\n\nNegative Binomial Regression is used to model count data with excess zeros (as in the Zero-Inflated Negative Binomial Regression) and is used to model rare events which are less likely to have counts where mean = variance. Negative Binomial can be extended to handle correlated/clustered data as well.\n\n## Assumptions\n\n-   The outcome represents count data\n-   The variance of the outcome is greater than its mean\n-   The relationship between the predictors and the log of the outcome's mean is linear\n-   The errors are independent of one another\n\n## Our Negative Binomial Regression Implementation\n\nOur Negative Binomial Regression implementation: (Note that we use bootstrapping to estimate standard errors)\n\n```{r}\nnegative_binomial_regression <- function(data, ..., y) {\n  n <- nrow(data)\n  x_parameters <- c(...)\n  # defining the predictor matrix\n  X <-\n    matrix(c(rep(1, n), x_parameters),\n      nrow = n,\n      ncol = ncol(data)\n    )\n  # defining the outcome matrix\n  Y <- matrix(y, nrow = n, ncol = 1)\n  # starting with theta = 1\n  theta <- 1\n  # defining the log likelihood\n  negative_binomial.likelihood <- function(beta, X, Y = y) {\n    eta <- X %*% beta\n    mu <- exp(eta)\n    loglikelihood <-\n      sum(Y * log(mu) - (Y + 1 / theta) * log(1 + mu / theta))\n    return(loglikelihood)\n  }\n  # starting with an initial guess of the parameter values\n  initial_guess <- rep(0, ncol(X))\n  # using 'optim' to maximize the log likelihood\n  result <- optim(\n    initial_guess,\n    negative_binomial.likelihood,\n    X = X,\n    Y = Y,\n    control = list(fnscale = -1),\n    hessian = T,\n    method = NULL\n  )$par\n  # creating a vector 'estimate' for the beta coefficients\n  estimate <- result\n  # bootstrapping to estimate the standard errors\n  num_bootstraps <- 10\n  result_bootstrap <-\n    matrix(0, nrow = num_bootstraps, ncol = ncol(X))\n  for (i in 1:num_bootstraps) {\n    sample_indices <- sample(nrow(data), replace = TRUE)\n    bootstrap_data <- data[sample_indices, ]\n    X_bootstrap <-\n      matrix(\n        c(rep(1, nrow(bootstrap_data)), x_parameters),\n        nrow = nrow(bootstrap_data),\n        ncol = ncol(bootstrap_data)\n      )\n    Y_bootstrap <-\n      matrix(bootstrap_data$y,\n        nrow = nrow(bootstrap_data),\n        ncol = 1\n      )\n    initial_guess_bootstrap <-\n      matrix(0, nrow = ncol(bootstrap_data), ncol = 1)\n    result_bootstrap[i, ] <- optim(\n      initial_guess_bootstrap,\n      negative_binomial.likelihood,\n      X = X_bootstrap,\n      Y = Y_bootstrap,\n      control = list(fnscale = -1),\n      hessian = T,\n      method = NULL\n    )$par\n  }\n  # finding the standard deviation of the bootstrapped betas to find the\n  # standard error of the coefficients\n  se <- apply(result_bootstrap, 2, sd)\n  # calculating the z-statistic\n  z <- estimate / se\n  # defining the degrees of freedom\n  df <- nrow(X) - ncol(X)\n  # calculating the p-value\n  p <- 2 * pnorm(z, lower.tail = FALSE)\n  # defining the row names of the output data frame\n  rownames <- c()\n  for (i in 1:((ncol(X)) - 1)) {\n    rownames[i] <- i\n  }\n  data_to_plot <- data[, -which(colnames(data) == \"y\")]\n  data_to_plot$y_log <- log(data$y)\n  test <- list(\n    pairs(data_to_plot, main = \"Assessing Linearity of Predictors \\nwith log of Outcome\")\n  )\n  impl <- data.frame(\n    Estimate = estimate,\n    Std.Error = se,\n    z.value = z,\n    p.value = p,\n    DegOfFreedom = c(df, rep(NA, ncol(X) - 1)),\n    row.names = c(\"(Intercept)\", paste0(rep(\"x\", ncol(\n      X\n    ) - 1), rownames))\n  )\n  # returning a data frame akin to the glm probit output\n  return(list(test, impl))\n}\n```\n\nCreating a function to predict the outcomes based on our Negative Binomial Regression implementation.\n\n```{r}\npredict_neg_binom <-\n  function(data, ..., y, implementation_neg_binom) {\n    n <-\n      implementation_neg_binom$DegOfFreedom[1] + nrow(implementation_neg_binom)\n    input_covariate_values <- c(...)\n    X <-\n      matrix(\n        c(rep(1, n), input_covariate_values),\n        nrow = n,\n        ncol = nrow(implementation_neg_binom)\n      )\n    Y <- matrix(y, nrow = n, ncol = 1)\n    estimate <-\n      implementation_neg_binom[1:nrow(implementation_neg_binom), 1]\n    pred <- exp(X %*% estimate)\n    return(pred)\n  }\n```\n\nCreating a test data set which meets all Negative Binomial Regression assumptions to check if our function works.\n\n```{r}\nx1 <- rnorm(100, mean = 0, sd = 0.5)\nx2 <- rnorm(100, mean = 0, sd = 0.5)\ny <- rnbinom(100, mu = exp(x1 + x2), size = 0.5)\ntest_neg_binom_regression_data <- data.frame(x1, x2, y)\n# to ensure that the variance of the outcome variable is greater\n# than its mean\nvar(y) > mean(y)\nplot(test_neg_binom_regression_data$x1, log(test_neg_binom_regression_data$y),\n  main = \"The relationship between the log of the outcome and x1 is linear (it is not apparent in this plot but our data structure captures this relationship)\", cex.main = 0.4,\n  xlab = \"x1\", ylab = \"y\"\n)\nplot(test_neg_binom_regression_data$x2, log(test_neg_binom_regression_data$y),\n  xlab = \"x2\", ylab = \"y\",\n  main = \"The relationship between the log of the outcome and x2 is linear (it is not apparent in this plot but our data structure captures this relationship)\", cex.main = 0.4\n)\n```\n\n## Testing Assumptions for Negative Binomial Regression\n\n```{r}\ntest_negbinom_reg <- negative_binomial_regression(test_neg_binom_regression_data,\n  test_neg_binom_regression_data$x1,\n  test_neg_binom_regression_data$x2,\n  y = test_neg_binom_regression_data$y\n)[[1]]\n```\n\nUsing our implementation of Negative Binomial to fit the model and get residual measure.\n\n```{r}\nour_implementation_neg_binom <-\n  negative_binomial_regression(\n    test_neg_binom_regression_data,\n    test_neg_binom_regression_data$x1,\n    test_neg_binom_regression_data$x2,\n    y = test_neg_binom_regression_data$y\n  )[[2]]\nour_implementation_neg_binom\n```\n\nComparing our output to R's output.\n\n```{r}\nr_implementation_neg_binom <-\n  summary(glm.nb(y ~ x1 + x2, data = test_neg_binom_regression_data))\nr_implementation_neg_binom\n```\n\nWe note that the results are similar.\n\nWe followed all assumptions of Negative Binomial Regression in regressing y on x1 and x2 using the test_neg_binom_regression_data data set. We will compare the residual of this regression to that of all the others where assumptions will be broken.\n\nThe residual for where all assumptions are met:\n\n```{r}\nprediction_all_assumptions_met <-\n  as.numeric(\n    predict_neg_binom(\n      test_neg_binom_regression_data,\n      test_neg_binom_regression_data$x1,\n      test_neg_binom_regression_data$x2,\n      y = test_neg_binom_regression_data$y,\n      implementation_neg_binom = our_implementation_neg_binom\n    )\n  )\nresidual_all_assumptions_met <- sqrt(mean((\n  test_neg_binom_regression_data$y - prediction_all_assumptions_met\n)^2))\nresidual_all_assumptions_met # small residual\n# residual plot\nplot(\n  test_neg_binom_regression_data$y - prediction_all_assumptions_met,\n  ylim = c(-30, 30),\n  ylab = \"Residuals\",\n  main = \"Residual Plot: All assumptions met\",\n  pch = 16\n)\nabline(\n  h = 0,\n  col = \"red\",\n  lty = 2,\n  lwd = 3\n)\n```\n\n## Breaking Assumptions\n\n### Breaking the assumption that the relationship between the predictors and the log of the outcome's mean is linear\n\nCreating a data set where, if we apply Negative Binomial regression, this assumption will be broken.\n\n```{r}\nx1 <- rnorm(100, mean = 0, sd = 0.5)\nx2 <- rnorm(100, mean = 0, sd = 0.5)\ny <- rnbinom(100, mu = exp(x1 + x2)^2, size = 0.5)\ntest_neg_binom_regression_data_not_linear <- data.frame(x1, x2, y)\n# to ensure that the variance of the outcome variable is greater\n# than its mean\nvar(y) > mean(y)\nplot(test_neg_binom_regression_data_not_linear$x1, log(test_neg_binom_regression_data_not_linear$y),\n  main = \"The relationship between the log of the outcome and x1 is not linear\", cex.main = 0.8,\n  xlab = \"x1\", ylab = \"y\"\n)\nplot(test_neg_binom_regression_data_not_linear$x2, log(test_neg_binom_regression_data_not_linear$y),\n  xlab = \"x2\", ylab = \"y\", cex.main = 0.8,\n  main = \"The relationship between the log of the outcome and x2 is not linear\"\n)\n```\n\nUsing our implementation of Negative Binomial to fit the model and get a residual measure.\n\n```{r}\nour_implementation_neg_binom_not_linear <-\n  negative_binomial_regression(\n    test_neg_binom_regression_data_not_linear,\n    test_neg_binom_regression_data_not_linear$x1,\n    test_neg_binom_regression_data_not_linear$x2,\n    y = test_neg_binom_regression_data_not_linear$y\n  )[[2]]\n\nprediction_not_linear <-\n  as.numeric(\n    predict_neg_binom(\n      test_neg_binom_regression_data_not_linear,\n      test_neg_binom_regression_data_not_linear$x1,\n      test_neg_binom_regression_data_not_linear$x2,\n      y = test_neg_binom_regression_data_not_linear$y,\n      implementation_neg_binom = our_implementation_neg_binom_not_linear\n    )\n  )\nresidual_not_linear <- sqrt(mean((\n  test_neg_binom_regression_data_not_linear$y - prediction_not_linear\n)^2))\nresidual_not_linear # large residual\n# residual plot\nplot(\n  test_neg_binom_regression_data_not_linear$y - prediction_not_linear,\n  ylim = c(-30, 30),\n  ylab = \"Residuals\",\n  main = \"Residual Plot: Linearity assumption violated\",\n  pch = 16\n)\nabline(\n  h = 0,\n  col = \"red\",\n  lty = 2,\n  lwd = 3\n)\n```\n\n### Breaking the assumption that the mean of the outcome is smaller than its variance\n\nCreating a data set where, if we apply Negative Binomial regression, this assumption will be broken.\n\n```{r}\nx1 <- rnorm(100, mean = 0, sd = 0.5)\nx2 <- rnorm(100, mean = 0, sd = 0.5)\ny <- rnbinom(100, mu = exp(x2 - 2 * x1), size = 100) + 10\ntest_neg_binom_regression_data_mean_greater <- data.frame(x1, x2, y)\n# to ensure that the variance of the outcome variable is smaller\n# than its mean\nvar(y) > mean(y)\n```\n\nUsing our implementation of Negative Binomial to fit the model and get a residual measure.\n\n```{r}\nour_implementation_neg_binom_mean_greater <-\n  negative_binomial_regression(\n    test_neg_binom_regression_data_mean_greater,\n    test_neg_binom_regression_data_mean_greater$x1,\n    test_neg_binom_regression_data_mean_greater$x2,\n    y = test_neg_binom_regression_data_mean_greater$y\n  )[[2]]\n\nprediction_mean_greater <-\n  as.numeric(\n    predict_neg_binom(\n      test_neg_binom_regression_data_mean_greater,\n      test_neg_binom_regression_data_mean_greater$x1,\n      test_neg_binom_regression_data_mean_greater$x2,\n      y = test_neg_binom_regression_data_mean_greater$y,\n      implementation_neg_binom = our_implementation_neg_binom_mean_greater\n    )\n  )\nresidual_mean_greater <- sqrt(mean((\n  test_neg_binom_regression_data_mean_greater$y - prediction_mean_greater\n)^2))\nresidual_mean_greater\n# residual plot\nplot(\n  test_neg_binom_regression_data_mean_greater$y - prediction_mean_greater,\n  ylim = c(-30, 30),\n  ylab = \"Residuals\",\n  cex.main = 0.9,\n  main = \"Residual Plot: Variance of outcome greater than mean assumption violated\",\n  pch = 16\n)\nabline(\n  h = 0,\n  col = \"red\",\n  lty = 2,\n  lwd = 3\n)\n```\n\n## Comparing residuals when all assumptions were met versus not\n\n```{r}\nresidual_comparison <-\n  t(\n    data.frame(\n      residual_all_assumptions_met,\n      residual_not_linear,\n      residual_mean_greater\n    )\n  )\nrow.names(residual_comparison) <- c(\n  \"All assumptions met\",\n  \"Linearity assumption violated\",\n  \"Variance > Mean assumption violated\"\n)\ncolnames(residual_comparison) <- \"Residuals\"\nresidual_comparison\n```\n\n## Conclusion\n\nThe implementation of Negative Binomial Regression where all assumptions are met performs well; however, even the model where an assumption is broken; i.e. where the mean of the outcome is greater than its variance, performs well too - however, it should be noted that even though its predictions might be accurate, its standard errors and p-values might be biased.\n\n```{r, message=F}\nset.seed(123)\nlibrary(alr4)\nlibrary(tidyverse)\nlibrary(MASS)\nlibrary(pscl)\nlibrary(glmbb) # for crabs data\nlibrary(kableExtra)\nlibrary(lmtest)\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"negative-binomial.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title":"Negative Binomial"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}