---
title: "DIY Regression"
author: "Jasmine Siswandjo and Saumya Seth"
date: "2023-12-17"
output: pdf_document
---

```{r}
set.seed(123)

library(tidyverse)
library(dplyr)
library(MASS)
```

# Linear Regression

## Introduction 

Linear Regression is one of the simplest regressions out there. In predicting an outcome from various covariate(s), it creates the 'best-fitting' line to the data that we observe to create a model - in that it predicts values on the line when given specific values of the covariates.

## Uses

Linear Regression is used across various fields. It is a model which has high 
bias and low variance. This means that even though it may not fit the data observed in the most optimal way (in that it may not be able to capture complexities in the data), it is not that sensitive to changes in the training data, which can make it more stable when dealing with small fluctuations or noise in the data set. Linear Regression can be used for predicting continuous, categorical, and even binary outcomes (as is often done in Causal Inference).

## Assumptions

- The predictors and the outcome are linearly related to one another
- The errors are normally distributed and are independent of one another
- The errors are homoscedastic

## Our Linear Regression Implementation

Our Linear Regression implementation: (Note that we use bootstrapping to estimate standard errors)

```{r}
linear_regression <- function(data, ..., y) {
  x_parameters <- c(...)
  n <- nrow(data)
  # defining the predictor matrix
  X <-
    matrix(c(rep(1, n), x_parameters),
      nrow = n,
      ncol = ncol(data)
    )
  # defining the outcome matrix
  Y <- matrix(y, nrow = n, ncol = 1)
  # solving for the beta coefficients
  beta <- solve(t(X) %*% X) %*% t(X) %*% Y
  # creating a vector 'estimate' for the beta coefficients
  estimate <- c()
  for (i in 1:ncol(X)) {
    estimate[i] <- beta[i]
  }
  # bootstrapping to estimate the standard errors
  num_bootstraps <- 10000
  bootstrap_betas <-
    matrix(0, nrow = num_bootstraps, ncol = ncol(data))
  for (i in 1:num_bootstraps) {
    sample_indices <- sample(nrow(data), replace = TRUE)
    bootstrap_data <- data[sample_indices, ]
    bootstrap_X <-
      as.matrix(cbind(1, bootstrap_data[, 1:(ncol(bootstrap_data) - 1)]))
    bootstrap_Y <- as.matrix(bootstrap_data$y, ncol = 1)
    bootstrap_beta <-
      solve(t(bootstrap_X) %*% bootstrap_X) %*% t(bootstrap_X) %*% bootstrap_Y
    bootstrap_betas[i, ] <- bootstrap_beta
  }
  # finding the standard deviation of the bootstrapped betas to find the
  # standard error of the coefficients
  se <- c()
  for (i in 1:ncol(X)) {
    se[i] <- apply(bootstrap_betas, 2, sd)[i]
  }
  # calculating the t-statistic
  t <- estimate / se
  # defining the degrees of freedom
  df <- nrow(X) - ncol(X)
  # calculating the p-value
  p <- 2 * pt(t, df, lower = F)
  # calculating the residuals
  residual <- sqrt(mean((Y - X %*% beta)^2))
  # defining the row names of the output data frame
  rownames <- c()
  for (i in 1:((ncol(X)) - 1)) {
    rownames[i] <- i
  }
  # returning a data frame akin to the lm output
  return(
    data.frame(
      Estimate = estimate,
      Std.Error = se,
      t.value = t,
      p.value = p,
      Residual = c(residual, rep(NA, ncol(X) - 1)),
      DegOfFreedom = c(df, rep(NA, ncol(X) - 1)),
      row.names = c("(Intercept)", paste0(rep("x", ncol(
        X
      ) - 1), rownames))
    )
  )
}
```

Creating a test data set which meets all Linear Regression assumptions to check if our function works.

```{r}
test_linear_regression_data <-
  data.frame(
    x1 = rnorm(100, mean = 5, sd = 2),
    x2 = rnorm(100, mean = 0, sd = 2)
  )
error <- rnorm(100, mean = 0, sd = 1) # errors are homoscedastic
test_linear_regression_data$y <-
  2 * test_linear_regression_data$x1 +
  0.2 * test_linear_regression_data$x2 + error

plot(test_linear_regression_data$x1, test_linear_regression_data$y,
  xlab = "x1", ylab = "y",
  main = "Outcome is linear to x1"
)
plot(test_linear_regression_data$x2, test_linear_regression_data$y,
  xlab = "x2", ylab = "y",
  main = "Outcome is linear to x2 (it is not apparent in this plot but our data structure captures this relationship)", cex.main = 0.6
)
plot(density(error), main = "Errors are normally distributed with mean 0")
plot(error,
  ylab = "residuals", main = "Residuals are homoscedastic", ylim = c(-3, 3)
)
```

Applying the function we created on this data set.

```{r}
our_implementation <- linear_regression(
  test_linear_regression_data,
  test_linear_regression_data$x1,
  test_linear_regression_data$x2,
  y = test_linear_regression_data$y
)
our_implementation
```

Comparing our output to R's output.

```{r}
r_implementation <-
  summary(lm(y ~ x1 + x2, data = test_linear_regression_data))
r_implementation
```

We note that the results are similar.

We followed all assumptions of Linear Regression in regressing y on x1 and x2 using the test_linear_regression_data data set. We will compare the residual of this regression to that of all the others where assumptions will be broken.

The residual for where all assumptions are met:

```{r}
our_implementation$Residual[1] # a small residual here
```

## Breaking Assumptions

### Breaking the assumption of the predictors and outcome following a linear relationship

Creating a data set where, if we apply linear regression, this assumption will be broken.

```{r}
test_linear_regression_data_not_linear <-
  data.frame(
    x1 = rnorm(100, mean = 5, sd = 2),
    x2 = rnorm(100, mean = 0, sd = 2)
  )
error <- rnorm(100, mean = 0, sd = 1)
test_linear_regression_data_not_linear$y <-
  2 * test_linear_regression_data_not_linear$x1^2 + 0.2 *
    test_linear_regression_data_not_linear$x2^2 + error

plot(test_linear_regression_data_not_linear$x1, test_linear_regression_data_not_linear$y,
  xlab = "x1", ylab = "y",
  main = "Outcome is not linear to x1"
)
plot(test_linear_regression_data_not_linear$x2, test_linear_regression_data_not_linear$y,
  xlab = "x2", ylab = "y",
  main = "Outcome is not linear to x2"
)
```

Using our implementation of Linear Regression to fit the model.

```{r}
our_implementation_not_linear <- linear_regression(
  test_linear_regression_data_not_linear,
  test_linear_regression_data_not_linear$x1,
  test_linear_regression_data_not_linear$x2,
  y = test_linear_regression_data_not_linear$y
)
our_implementation_not_linear
our_implementation_not_linear$Residual[1] # a higher residual here
```

We note that linear regression is not performing as well in this case.

### Breaking the assumption of the errors being normally distributed

Creating a data set where, if we apply linear regression, this assumption will be broken.

```{r}
test_linear_regression_data_not_normally_dist <-
  data.frame(
    x1 = rnorm(100, mean = 5, sd = 2),
    x2 = rnorm(100, mean = 0, sd = 2)
  )
error <- runif(100, min = 0, max = 5)
test_linear_regression_data_not_normally_dist$y <-
  2 * test_linear_regression_data_not_normally_dist$x1 + 0.2 *
    test_linear_regression_data_not_normally_dist$x2 + error

plot(density(error), main = "Errors are not normally distributed")
```

Using our implementation of lm to fit the model.

```{r}
our_implementation_not_normally_dist <- linear_regression(
  test_linear_regression_data_not_normally_dist,
  test_linear_regression_data_not_normally_dist$x1,
  test_linear_regression_data_not_normally_dist$x2,
  y = test_linear_regression_data_not_normally_dist$y
)
our_implementation_not_normally_dist
our_implementation_not_normally_dist$Residual[1] # a higher residual here
```

We note that linear regression is not performing as well in this case.

### Breaking the assumption of the errors being homoscedastic

Creating a data set where, if we apply linear regression, this assumption will be broken.

```{r}
test_linear_regression_data_not_homoscedastic <-
  data.frame(
    x1 = rnorm(100, mean = 5, sd = 2),
    x2 = rnorm(100, mean = 0, sd = 2)
  )
error <- c(
  rnorm(50, mean = 0, sd = 1),
  rnorm(50, mean = 0, sd = 10)
)
test_linear_regression_data_not_homoscedastic$y <-
  2 * test_linear_regression_data_not_homoscedastic$x1 + 0.2 *
    test_linear_regression_data_not_homoscedastic$x2 + error

plot(error,
  ylab = "error", main = "Residuals are not homoscedastic", ylim = c(-20, 20)
)
```

Using our implementation of lm to fit the model.

```{r}
our_implementation_not_homoscedastic <- linear_regression(
  test_linear_regression_data_not_homoscedastic,
  test_linear_regression_data_not_homoscedastic$x1,
  test_linear_regression_data_not_homoscedastic$x2,
  y = test_linear_regression_data_not_homoscedastic$y
)
our_implementation_not_homoscedastic
our_implementation_not_homoscedastic$Residual[1] # a higher residual here
```

We note that linear regression is not performing as well in this case.

## Comparing residuals when all assumptions were met versus not

```{r}
residual_comparison <-
  t(
    data.frame(
      resid_all_assumptions_met = our_implementation$Residual[1],
      resid_not_linear = our_implementation_not_linear$Residual[1],
      resid_not_normally_dist = our_implementation_not_normally_dist$Residual[1],
      resid_not_homoscedastic = our_implementation_not_homoscedastic$Residual[1]
    )
  )
row.names(residual_comparison) <- c(
  "All assumptions met",
  "Linearity assumption violated",
  "Normality assumption violated",
  "Homoscedasticity assumption violated"
)
colnames(residual_comparison) <- "Residuals"
residual_comparison
```

## Conclusion 

The implementation of Linear Regression where all assumptions are met performs the best; i.e. it gives us predictions which are closest to the true outcome values. From the residual comparison, we also note that applying linear regression to data that aren't linear can be especially worrisome.

# Probit Regression

## Introduction 

The Probit model classifies observations into one of two categories (for simple Probit Regression; multinomial Probit Regression can classify observations into more than two categories) by estimating the probability that an observation with particular characteristics is more likely to fall in one category or another.

## Uses

Probit Regression is primarily used when the outcome is binary - thus, it is mainly used for classification problems. When covariates are continuous, there are infinite possible values for the outcome if using Linear Regression; Logistic and Probit Regressions are therefore better than Linear if we need to bound the outcome to 0 and 1.

Logistic Regression and Probit Regressions give almost identical results - they just have different link functions. The decision to chose one over the other is discipline-dependent, and it is said that Logistic Regression is better when one has extreme independent variables (where one particular small or large value will overwhelmingly determine if your outcome is 0 or 1 - overriding the effect of most other variables). However, there is no 'right' answer to this debate.

## Assumptions

- The outcome is binary
- The z-score of the outcome and the predictor variables have a linear relationship
- The errors are normally distributed and are independent of one another

## Our Probit Regression Implementation

Our Probit Regression implementation: (Note that we use bootstrapping to estimate standard errors)

```{r}
probit_regression <- function(data, ..., y) {
  n <- nrow(data)
  x_parameters <- c(...)
  # defining the predictor matrix
  X <-
    matrix(c(rep(1, n), x_parameters),
      nrow = n,
      ncol = ncol(data)
    )
  # defining the outcome matrix
  Y <- matrix(y, nrow = n, ncol = 1)
  # defining the log likelihood
  probit.loglikelihood <- function(beta, X, Y) {
    eta <- X %*% beta
    p <- pnorm(eta)
    loglikelihood <- -sum((1 - Y) * log(1 - p) + Y * log(p))
    return(loglikelihood)
  }
  # starting with an initial guess of the parameter values
  initial_guess <- matrix(0, nrow = ncol(data), ncol = 1)
  # using 'optim' to maximize the log likelihood
  result <- optim(
    initial_guess,
    fn = probit.loglikelihood,
    X = X,
    Y = Y,
    method = NULL
  )$par
  # creating a vector 'estimate' for the beta coefficients
  estimate <- result
  # bootstrapping to estimate the standard errors
  num_bootstraps <- 10000
  result_bootstrap <-
    matrix(0, nrow = num_bootstraps, ncol = ncol(X))
  for (i in 1:num_bootstraps) {
    sample_indices <- sample(nrow(data), replace = TRUE)
    bootstrap_data <- data[sample_indices, ]
    X_bootstrap <-
      matrix(
        c(rep(1, nrow(bootstrap_data)), x_parameters),
        nrow = nrow(bootstrap_data),
        ncol = ncol(bootstrap_data)
      )
    Y_bootstrap <-
      matrix(bootstrap_data$y,
        nrow = nrow(bootstrap_data),
        ncol = 1
      )
    initial_guess_bootstrap <-
      matrix(0, nrow = ncol(bootstrap_data), ncol = 1)
    result_bootstrap[i, ] <- optim(
      initial_guess_bootstrap,
      probit.loglikelihood,
      X = X_bootstrap,
      Y = Y_bootstrap,
      method = NULL
    )$par
  }
  # finding the standard deviation of the bootstrapped betas to find the
  # standard error of the coefficients
  se <- apply(result_bootstrap, 2, sd)
  # calculating the z-statistic
  z <- estimate / se
  # defining the degrees of freedom
  df <- nrow(X) - ncol(X)
  # calculating the p-value
  p <- 2 * pnorm(z, lower.tail = FALSE)
  # defining the row names of the output data frame
  rownames <- c()
  for (i in 1:((ncol(X)) - 1)) {
    rownames[i] <- i
  }
  # returning a data frame akin to the glm probit output
  return(
    data.frame(
      Estimate = estimate,
      Std.Error = se,
      z.value = z,
      p.value = p,
      DegOfFreedom = c(df, rep(NA, ncol(X) - 1)),
      row.names = c("(Intercept)", paste0(rep("x", ncol(
        X
      ) - 1), rownames))
    )
  )
}
```

Creating a function to predict the outcomes based on our Probit Regression implementation.

```{r}
predict_probit <-
  function(data, ..., y, implementation_probit) {
    n <-
      implementation_probit$DegOfFreedom[1] + nrow(implementation_probit)
    input_covariate_values <- c(...)
    X <-
      matrix(
        c(rep(1, n), input_covariate_values),
        nrow = n,
        ncol = nrow(implementation_probit)
      )
    Y <- matrix(y, nrow = n, ncol = 1)
    estimate <-
      implementation_probit[1:nrow(implementation_probit), 1]
    pred <- ifelse(X %*% estimate < 0, 0, 1)
    return(pred)
  }
```

Creating a test data set which meets all Probit Regression assumptions to check if our function works.

```{r}
test_probit_regression_data <- data.frame(
  x1 = rnorm(1000, 0, 1),
  x2 = rnorm(1000, 0, 1)
)
error <- rnorm(1000, mean = 0, sd = 0.5)
test_probit_regression_data$y <- test_probit_regression_data$x1 +
  0.5 * test_probit_regression_data$x2 +
  error
test_probit_regression_data$y <-
  qnorm(pnorm(test_probit_regression_data$y))

plot(test_probit_regression_data$x1, test_probit_regression_data$y,
  main = "The z score of y and x1 have a linear relationship", cex.main = 0.6,
  xlab = "x1", ylab = "y"
)
plot(test_probit_regression_data$x1, test_probit_regression_data$y,
  main = "The z score of y and x2 have a linear relationship", cex.main = 0.6,
  xlab = "x2", ylab = "y"
)

test_probit_regression_data$y <-
  ifelse(test_probit_regression_data$y < 0, 0, 1)

plot(density(error), main = "Errors are normally distributed")
```

Applying the function we created on this data set.

```{r}
our_implementation_probit <-
  probit_regression(
    test_probit_regression_data,
    test_probit_regression_data$x1,
    test_probit_regression_data$x2,
    y = test_probit_regression_data$y
  )
our_implementation_probit
```

Comparing our output to R's output.

```{r}
r_implementation_probit <-
  summary(glm(y ~ x1 + x2,
    data = test_probit_regression_data,
    family = binomial(link = "probit")
  ))
r_implementation_probit
```

We note that the results are similar.

We followed all assumptions of Probit Regression in regressing y on x1 and x2 using the test_probit_regression_data data set. We will compare the residual of this regression to that of all the others where assumptions will be broken.

The accuracy for where all assumptions are met:

```{r}
prediction_all_assumptions_met <-
  as.numeric(
    predict_probit(
      test_probit_regression_data,
      test_probit_regression_data$x1,
      test_probit_regression_data$x2,
      y = test_probit_regression_data$y,
      implementation_probit = our_implementation_probit
    )
  )
accuracy_all_assumptions_met <-
  sum(prediction_all_assumptions_met == test_probit_regression_data$y) / 1000
accuracy_all_assumptions_met # high accuracy here
```

## Breaking Assumptions

### Breaking the assumption that the relationship between the predictors and the z score of y is linear

Creating a data set where, if we apply Probit Regression, this assumption will be broken.

```{r}
test_probit_regression_data_not_linear <-
  data.frame(
    x1 = rnorm(1000, 0, 1),
    x2 = rnorm(1000, 0, 1)
  )
eror <- rnorm(1000, mean = 0, sd = 0.5)
test_probit_regression_data_not_linear$y <-
  test_probit_regression_data_not_linear$x1^2 + error
test_probit_regression_data_not_linear$y <-
  qnorm(pnorm(test_probit_regression_data_not_linear$y))

plot(test_probit_regression_data_not_linear$x1, test_probit_regression_data_not_linear$y,
  main = "The z score of y and x1 do not have a linear relationship",
  xlab = "x1", ylab = "y"
)

test_probit_regression_data_not_linear$y <-
  ifelse(test_probit_regression_data_not_linear$y < 0, 0, 1)
```

Using our implementation of glm Probit to fit the model and get an accuracy measure.

```{r}
our_implementation_probit_not_linear <-
  probit_regression(
    test_probit_regression_data_not_linear,
    test_probit_regression_data_not_linear$x1,
    test_probit_regression_data_not_linear$x2,
    y = test_probit_regression_data_not_linear$y
  )
our_implementation_probit_not_linear

prediction_not_linear <-
  as.numeric(
    predict_probit(
      test_probit_regression_data_not_linear,
      test_probit_regression_data_not_linear$x1,
      test_probit_regression_data_not_linear$x2,
      y = test_probit_regression_data_not_linear$y,
      implementation_probit = our_implementation_probit_not_linear
    )
  )
accuracy_not_linear <-
  sum(prediction_not_linear == test_probit_regression_data_not_linear$y) / 1000
accuracy_not_linear # lower accuracy here
```

We note that Probit Regression is not performing as well in this case.

### Breaking the assumption that the errors are normally distributed

Creating a data set where, if we apply Probit Regression, this assumption will be broken.

```{r}
test_probit_regression_data_not_normally_dist <-
  data.frame(
    x1 = rnorm(1000, 0, 1),
    x2 = rnorm(1000, 0, 1)
  )
error <- runif(1000, min = -1, max = 1)
test_probit_regression_data_not_normally_dist$y <-
  test_probit_regression_data_not_normally_dist$x1 + error
test_probit_regression_data_not_normally_dist$y <-
  qnorm(pnorm(test_probit_regression_data_not_normally_dist$y))

plot(density(error), main = "Errors are not normally distributed")

test_probit_regression_data_not_normally_dist$y <-
  ifelse(test_probit_regression_data_not_normally_dist$y < 0, 0, 1)
```

Using our implementation of glm Probit to fit the model and get an accuracy measure.

```{r}
our_implementation_probit_not_normally_dist <-
  probit_regression(
    test_probit_regression_data_not_normally_dist,
    test_probit_regression_data_not_normally_dist$x1,
    test_probit_regression_data_not_normally_dist$x2,
    y = test_probit_regression_data_not_normally_dist$y
  )
our_implementation_probit_not_normally_dist

prediction_not_normally_dist <-
  as.numeric(
    predict_probit(
      test_probit_regression_data_not_normally_dist,
      test_probit_regression_data_not_normally_dist$x1,
      test_probit_regression_data_not_normally_dist$x2,
      y = test_probit_regression_data_not_normally_dist$y,
      implementation_probit = our_implementation_probit_not_normally_dist
    )
  )
accuracy_not_normally_dist <-
  sum(prediction_not_normally_dist == test_probit_regression_data_not_normally_dist$y) / 1000
accuracy_not_normally_dist # lower accuracy here
```

We note that Probit Regression is not performing as well in this case.

## Comparing accuracies when all assumptions were met versus not

```{r}
accuracy_comparison <-
  t(
    data.frame(
      accuracy_all_assumptions_met,
      accuracy_not_linear,
      accuracy_not_normally_dist
    )
  )
row.names(accuracy_comparison) <- c(
  "All assumptions met",
  "Linearity assumption violated",
  "Normality assumption violated"
)
colnames(accuracy_comparison) <- "Accuracy"
accuracy_comparison
```

## Conclusion

The implementation of Probit Regression where all assumptions are met performs the best; i.e. it gives us predictions which are more accurate to the true outcome values.

# Negative Binomial Regression

## Introduction 

Negative Binomial Regression is used for predicting count data, similar to Poisson Regression, but the Negative Binomial is more flexible as it allows for the variance of the outcome to be greater than its mean (in Poisson Regression, they are assumed to be equal).

## Uses

Negative Binomial Regression is used to model count data with excess zeros (as in the Zero-Inflated Negative Binomial Regression) and is used to model rare events which are less likely to have counts where mean = variance. Negative Binomial can be extended to handle correlated/clustered data as well.

## Assumptions

- The outcome represents count data
- The variance of the outcome is greater than its mean
- The relationship between the predictors and the log of the outcome's mean is linear
- The errors are independent of one another

## Our Negative Binomial Regression Implementation

Our Negative Binomial Regression implementation: (Note that we use bootstrapping to estimate standard errors)

```{r}
negative_binomial_regression <- function(data, ..., y) {
  n <- nrow(data)
  x_parameters <- c(...)
  # defining the predictor matrix
  X <-
    matrix(c(rep(1, n), x_parameters),
      nrow = n,
      ncol = ncol(data)
    )
  # defining the outcome matrix
  Y <- matrix(y, nrow = n, ncol = 1)
  # starting with theta = 1
  theta <- 1
  # defining the log likelihood
  negative_binomial.likelihood <- function(beta, X, Y = y) {
    eta <- X %*% beta
    mu <- exp(eta)
    loglikelihood <-
      sum(Y * log(mu) - (Y + 1 / theta) * log(1 + mu / theta))
    return(loglikelihood)
  }
  # starting with an initial guess of the parameter values
  initial_guess <- rep(0, ncol(X))
  # using 'optim' to maximize the log likelihood
  result <- optim(
    initial_guess,
    negative_binomial.likelihood,
    X = X,
    Y = Y,
    control = list(fnscale = -1),
    hessian = T,
    method = NULL
  )$par
  # creating a vector 'estimate' for the beta coefficients
  estimate <- result
  # bootstrapping to estimate the standard errors
  num_bootstraps <- 10000
  result_bootstrap <-
    matrix(0, nrow = num_bootstraps, ncol = ncol(X))
  for (i in 1:num_bootstraps) {
    sample_indices <- sample(nrow(data), replace = TRUE)
    bootstrap_data <- data[sample_indices, ]
    X_bootstrap <-
      matrix(
        c(rep(1, nrow(bootstrap_data)), x_parameters),
        nrow = nrow(bootstrap_data),
        ncol = ncol(bootstrap_data)
      )
    Y_bootstrap <-
      matrix(bootstrap_data$y,
        nrow = nrow(bootstrap_data),
        ncol = 1
      )
    initial_guess_bootstrap <-
      matrix(0, nrow = ncol(bootstrap_data), ncol = 1)
    result_bootstrap[i, ] <- optim(
      initial_guess_bootstrap,
      negative_binomial.likelihood,
      X = X_bootstrap,
      Y = Y_bootstrap,
      control = list(fnscale = -1),
      hessian = T,
      method = NULL
    )$par
  }
  # finding the standard deviation of the bootstrapped betas to find the
  # standard error of the coefficients
  se <- apply(result_bootstrap, 2, sd)
  # calculating the z-statistic
  z <- estimate / se
  # defining the degrees of freedom
  df <- nrow(X) - ncol(X)
  # calculating the p-value
  p <- 2 * pnorm(z, lower.tail = FALSE)
  # defining the row names of the output data frame
  rownames <- c()
  for (i in 1:((ncol(X)) - 1)) {
    rownames[i] <- i
  }
  # returning a data frame akin to the glm probit output
  return(
    data.frame(
      Estimate = estimate,
      Std.Error = se,
      z.value = z,
      p.value = p,
      DegOfFreedom = c(df, rep(NA, ncol(X) - 1)),
      row.names = c("(Intercept)", paste0(rep("x", ncol(
        X
      ) - 1), rownames))
    )
  )
}
```

Creating a function to predict the outcomes based on our Negative Binomial Regression implementation.

```{r}
predict_neg_binom <-
  function(data, ..., y, implementation_neg_binom) {
    n <-
      implementation_neg_binom$DegOfFreedom[1] + nrow(implementation_neg_binom)
    input_covariate_values <- c(...)
    X <-
      matrix(
        c(rep(1, n), input_covariate_values),
        nrow = n,
        ncol = nrow(implementation_neg_binom)
      )
    Y <- matrix(y, nrow = n, ncol = 1)
    estimate <-
      implementation_neg_binom[1:nrow(implementation_neg_binom), 1]
    pred <- exp(X %*% estimate)
    return(pred)
  }
```

Creating a test data set which meets all Negative Binomial Regression assumptions to check if our function works.

```{r}
x1 <- rnorm(100, mean = 0, sd = 0.5)
x2 <- rnorm(100, mean = 0, sd = 0.5)
y <- rnbinom(100, mu = exp(x1 + x2), size = 0.5)
test_neg_binom_regression_data <- data.frame(x1, x2, y)
# to ensure that the variance of the outcome variable is greater
# than its mean
var(y) > mean(y)
plot(test_neg_binom_regression_data$x1, log(test_neg_binom_regression_data$y),
  main = "The relationship between the log of the outcome and x1 is linear (it is not apparent in this plot but our data structure captures this relationship)", cex.main = 0.4,
  xlab = "x1", ylab = "y"
)
plot(test_neg_binom_regression_data$x2, log(test_neg_binom_regression_data$y),
  xlab = "x2", ylab = "y",
  main = "The relationship between the log of the outcome and x2 is linear (it is not apparent in this plot but our data structure captures this relationship)", cex.main = 0.4
)
```

Using our implementation of Negative Binomial to fit the model and get residual measure.

```{r}
our_implementation_neg_binom <-
  negative_binomial_regression(
    test_neg_binom_regression_data,
    test_neg_binom_regression_data$x1,
    test_neg_binom_regression_data$x2,
    y = test_neg_binom_regression_data$y
  )
our_implementation_neg_binom
```

Comparing our output to R's output.

```{r}
r_implementation_neg_binom <-
  summary(glm.nb(y ~ x1 + x2, data = test_neg_binom_regression_data))
r_implementation_neg_binom
```

We note that the results are similar.

We followed all assumptions of Negative Binomial Regression in regressing y on x1 and x2 using the test_neg_binom_regression_data data set. We will compare the residual of this regression to that of all the others where assumptions will be broken.

The residual for where all assumptions are met:

```{r}
prediction_all_assumptions_met <-
  as.numeric(
    predict_neg_binom(
      test_neg_binom_regression_data,
      test_neg_binom_regression_data$x1,
      test_neg_binom_regression_data$x2,
      y = test_neg_binom_regression_data$y,
      implementation_neg_binom = our_implementation_neg_binom
    )
  )
residual_all_assumptions_met <- sqrt(mean((
  test_neg_binom_regression_data$y - prediction_all_assumptions_met
)^2))
residual_all_assumptions_met # small residual
# residual plot
plot(
  test_neg_binom_regression_data$y - prediction_all_assumptions_met,
  ylim = c(-30, 30),
  ylab = "Residuals",
  main = "Residual Plot: All assumptions met",
  pch = 16
)
abline(
  h = 0,
  col = "red",
  lty = 2,
  lwd = 3
)
```

## Breaking Assumptions

### Breaking the assumption that the relationship between the predictors and the log of the outcome's mean is linear

Creating a data set where, if we apply Negative Binomial regression, this assumption will be broken.

```{r}
x1 <- rnorm(100, mean = 0, sd = 0.5)
x2 <- rnorm(100, mean = 0, sd = 0.5)
y <- rnbinom(100, mu = exp(x1 + x2)^2, size = 0.5)
test_neg_binom_regression_data_not_linear <- data.frame(x1, x2, y)
# to ensure that the variance of the outcome variable is greater
# than its mean
var(y) > mean(y)
plot(test_neg_binom_regression_data_not_linear$x1, log(test_neg_binom_regression_data_not_linear$y),
  main = "The relationship between the log of the outcome and x1 is not linear", cex.main = 0.8,
  xlab = "x1", ylab = "y"
)
plot(test_neg_binom_regression_data_not_linear$x2, log(test_neg_binom_regression_data_not_linear$y),
  xlab = "x2", ylab = "y", cex.main = 0.8,
  main = "The relationship between the log of the outcome and x2 is not linear"
)
```

Using our implementation of Negative Binomial to fit the model and get a residual measure.

```{r}
our_implementation_neg_binom_not_linear <-
  negative_binomial_regression(
    test_neg_binom_regression_data_not_linear,
    test_neg_binom_regression_data_not_linear$x1,
    test_neg_binom_regression_data_not_linear$x2,
    y = test_neg_binom_regression_data_not_linear$y
  )
our_implementation_neg_binom_not_linear

prediction_not_linear <-
  as.numeric(
    predict_neg_binom(
      test_neg_binom_regression_data_not_linear,
      test_neg_binom_regression_data_not_linear$x1,
      test_neg_binom_regression_data_not_linear$x2,
      y = test_neg_binom_regression_data_not_linear$y,
      implementation_neg_binom = our_implementation_neg_binom_not_linear
    )
  )
residual_not_linear <- sqrt(mean((
  test_neg_binom_regression_data_not_linear$y - prediction_not_linear
)^2))
residual_not_linear # large residual
# residual plot
plot(
  test_neg_binom_regression_data_not_linear$y - prediction_not_linear,
  ylim = c(-30, 30),
  ylab = "Residuals",
  main = "Residual Plot: Linearity assumption violated",
  pch = 16
)
abline(
  h = 0,
  col = "red",
  lty = 2,
  lwd = 3
)
```

### Breaking the assumption that the mean of the outcome is smaller than its variance

Creating a data set where, if we apply Negative Binomial regression, this assumption will be broken.

```{r}
x1 <- rnorm(100, mean = 0, sd = 0.5)
x2 <- rnorm(100, mean = 0, sd = 0.5)
y <- rnbinom(100, mu = exp(x2 - 2 * x1), size = 100) + 10
test_neg_binom_regression_data_mean_greater <- data.frame(x1, x2, y)
# to ensure that the variance of the outcome variable is smaller
# than its mean
var(y) > mean(y)
```

Using our implementation of Negative Binomial to fit the model and get a residual measure.

```{r}
our_implementation_neg_binom_mean_greater <-
  negative_binomial_regression(
    test_neg_binom_regression_data_mean_greater,
    test_neg_binom_regression_data_mean_greater$x1,
    test_neg_binom_regression_data_mean_greater$x2,
    y = test_neg_binom_regression_data_mean_greater$y
  )
our_implementation_neg_binom_mean_greater

prediction_mean_greater <-
  as.numeric(
    predict_neg_binom(
      test_neg_binom_regression_data_mean_greater,
      test_neg_binom_regression_data_mean_greater$x1,
      test_neg_binom_regression_data_mean_greater$x2,
      y = test_neg_binom_regression_data_mean_greater$y,
      implementation_neg_binom = our_implementation_neg_binom_mean_greater
    )
  )
residual_mean_greater <- sqrt(mean((
  test_neg_binom_regression_data_mean_greater$y - prediction_mean_greater
)^2))
residual_mean_greater
# residual plot
plot(
  test_neg_binom_regression_data_mean_greater$y - prediction_mean_greater,
  ylim = c(-30, 30),
  ylab = "Residuals",
  cex.main = 0.9,
  main = "Residual Plot: Variance of outcome greater than mean assumption violated",
  pch = 16
)
abline(
  h = 0,
  col = "red",
  lty = 2,
  lwd = 3
)
```

## Comparing residuals when all assumptions were met versus not

```{r}
residual_comparison <-
  t(
    data.frame(
      residual_all_assumptions_met,
      residual_not_linear,
      residual_mean_greater
    )
  )
row.names(residual_comparison) <- c(
  "All assumptions met",
  "Linearity assumption violated",
  "Variance > Mean assumption violated"
)
colnames(residual_comparison) <- "Residuals"
residual_comparison
```

## Conclusion

The implementation of Negative Binomial Regression where all assumptions are met performs well; however, even the model where an assumption is broken; i.e. where the mean of the outcome is greater than its variance, performs well too - however, it should be noted that even though its predictions might be accurate, its standard errors and p-values might be biased.

# A Note

In determining how appropriate a particular regression model is for a problem, we think it might be valuable to move beyond a 'assumption #1 met' or 'assumption #1 not met' assessment. Instead, it might be more helpful for researchers to explore their data, and test assumptions on their own to determine 'what level of assumption-violation' might they be willing to accept given they are experts in their fields and have domain-specific contextual knowledge. This is not to say that assumptions can be violated without consequences.

Also, it might be difficult to determine the thresholds for having actually met or not met an assumption. Thus, a function which simply outputs a 'yes' or 'no' to whether assumptions for a particular regression implementation have been met did not seem appropriate. We therefore, test assumptions by intentionally breaking them and noting the extent of bias-ness that they result. 
