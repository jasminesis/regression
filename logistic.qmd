---
title: "Logistic"
---

```{r, message=F, echo=F}
library(knitr)
opts_chunk$set(fig.width = 5, fig.height = 4, fig.align = "left", out.width = "8.5in")

set.seed(123)
library(tidyverse)
library(dplyr)
library(MASS)
library(alr4)
library(pscl)
library(glmbb) # for crabs data
library(kableExtra)
library(lmtest)
```


## Introduction

Logistic regression is used when the outcome variable is discrete and binary, which is called classification. Multinomial logistic regression can classify observations into more than two categories, but we are only doing simple logistic regression here, with two categories. We use the inverse logit function to model the probability that $Y_i = 1$.

$$
logit^{-1}(x)=\frac{e^x}{1+e^x} \\
Pr(y_i=1) = logit^{-1}(X_i\beta)
$$`plogis` is the invlogit function.

## Uses

Logistic regression is good for when covariates are continuous, as the outcome variables are bounded between 0 and 1, through the logit link.

## Assumptions

1.  For binary logistic regression, that outcome variables are binary
2.  Independence of errors
3.  Linear relationship between the outcome variable and log odds of the predictor variables
4.  No multicollinearity

## Our Logistic Regression Implementation

```{r}
library(alr4)
# invlogit <- plogis

logistic_function <- function(fn_formula, data, predict = F) {
  number_omitted <- nrow(data) - nrow(na.omit(data))
  data <- na.omit(data)

  vars <- all.vars(as.formula(fn_formula))
  y_name <- vars[1]
  x_name <- vars[2:length(vars)]
  n <- nrow(data)
  Y <- matrix(data[, y_name], nrow = n, ncol = 1)
  X <- matrix(cbind(rep(1, n)))

  # take in categorical data
  var_names <- vector("character")
  for (i in x_name) {
    if (suppressWarnings(all(!is.na(as.numeric(as.character(data[, i])))))) {
      X <- cbind(X, as.numeric(as.character(data[, i])))
      var_names <- c(var_names, i)
    } else {
      categories <- sort(unique(data[, i]))
      for (j in categories[2:length(categories)]) {
        new_col_name <- paste0(i, j)
        new_col <- ifelse(data[, i] == j, 1, 0)
        X <- cbind(X, new_col)
        var_names <- c(var_names, new_col_name)
      }
    }
  }
  optim_logistic <- function(beta, X, Y) {
    beta <- as.matrix(beta, nrow = 4)
    pi <- plogis(X %*% beta)
    loglikelihood <- -sum(Y * log(pi) + (1 - Y) * log(1 - pi))
    return(loglikelihood)
  }
  result <- optim(par = rep(0, ncol(X)), fn = optim_logistic, X = X, Y = Y, hessian = T)
  OI <- solve(result$hessian)
  se <- sqrt(diag(OI))
  t_statistic <- result$par / se
  df <- nrow(X) - ncol(X)
  p_value <- 2 * pnorm(-1 * abs(t_statistic))
  # https://stats.stackexchange.com/questions/52475/how-are-the-p-values-of-the-glm-in-r-calculated

  coef <- rbind(result$par, se, t_statistic, p_value)
  colnames(coef) <- c("(Intercept)", var_names)
  rownames(coef) <- c("Estimate", "Std. Error", "z value", "p value")
  coef <- t(coef)

  b_hat <- result$par
  predictions <- plogis(X %*% b_hat)

  if (predict) {
    return(predictions)
  } else {
    return(coef)
  }
}
```

Creating testing data set with a single predictor variable to compare our implementation with `glm` logistic function

```{r}
# create fake data
x1 <- rnorm(100, 2, 1)
prob <- plogis(-1 + 0.5 * x1)
y <- rbinom(100, 1, prob)
sim_data <- data.frame(y, x1)

# compare DIY logistic function with glm
fit_sim_data_1 <- glm(y ~ x1, data = sim_data, family = binomial)
summary(fit_sim_data_1)$coef
logistic_function(fn_formula = "y ~ x1", data = sim_data)

# checking for linear relationship
plot(sim_data$x1, prob,
  main = "The log odds of y and x1 have a linear relationship", cex.main = 0.6,
  xlab = "x1", ylab = "y"
)

# check for correlation of residuals vs fit
plot(fit_sim_data_1, which = 1)
```

Creating testing data set with multiple covariates to compare our implementation with `glm` logistic function

```{r}
# create fake data with multiple x's
x1 <- rnorm(100, 2, 1)
x2 <- rnorm(100, 4, 1)
x3 <- rnorm(100, 6, 1)
prob <- plogis(-1 + x1 + x2 - 0.5 * x3)
y <- rbinom(100, 1, prob)
sim_data <- data.frame(y, x1, x2, x3)

# compare DIY logistic function with glm
fit_sim_data <- glm(y ~ x1 + x2 + x3, data = sim_data, family = binomial)
summary(fit_sim_data)$coef
logistic_function(fn_formula = "y ~ x1 + x2 + x3", data = sim_data)
```

Using the alr4 Donner data to test categorical data, and compare our implementation with `glm` logistic function

```{r}
Donner$survived <- Donner$y == "survived"
fit_Donner <- glm(survived ~ age + sex + status, data = Donner, family = "binomial")
summary(fit_Donner)$coef
logistic_function(fn_formula = "survived ~ age + sex + status", data = Donner)
```

**Interpretation of the coefficients**

A 1-unit difference in age corresponds to -0.02 in the logit probability of having survived in the Donner party, or a multiplicative change of $e^{-0.0283}=0.972$ in the odds of surviving.

```{r}
ggplot(Donner) +
  geom_jitter(aes(x = age, y = survived, color = survived)) +
  facet_wrap(vars(status)) +
  ggtitle("Younger people generally had a higher chance of surviving")
```

Show that our implementation of logistic regression can also make predictions

```{r}
idx <- sample(1:nrow(Donner), 5)
p1 <- logistic_function(fn_formula = "survived ~ age + sex + status", data = Donner, predict = T)[idx, ]
p2 <- predict(fit_Donner, type = "response")[idx]
compare_predict_data <- data.frame(p1, p2)
colnames(compare_predict_data) <- c("Our implementation", "GLM")

kable(compare_predict_data, digits = 3, caption = "Comparison of logistic prediction", booktabs = TRUE, valign = "t") |> kable_styling(latex_options = "HOLD_position")
```

## Function to check assumptions

```{r}
test_logistic_assumptions <- function(fn_formula, data) {
  n <- nrow(data)
  vars <- all.vars(as.formula(fn_formula))
  y_name <- vars[1]
  Y <- data[, y_name]

  # outcome variables are binary
  if (length(unique(Y)) == 2) {
    assp_1 <- paste("Binary outcomes assumption is met.")
  } else {
    return(paste("Binary outcomes assumption is not satisfied. There are", length(unique(Y)), "outcomes."))
  }

  x_name <- vars[2:length(vars)]
  X <- data[, x_name]
  preds <- logistic_function(fn_formula = fn_formula, data = data, predict = T) # predictions are in probability
  logit_vals <- log(preds / (1 - preds))
  plot_data <- data.frame(logit_vals, X) |> gather(key = "predictors", value = "predictor_value", -logit_vals)

  # Linear relationship between the outcome variable and log odds of the predictor variables
  assp_2 <- ggplot(plot_data, aes(logit_vals, predictor_value)) +
    geom_point(size = 0.5, alpha = 0.5) +
    geom_smooth(method = "loess") +
    theme_bw() +
    facet_wrap(~predictors, scales = "free_y")

  # Independence of errors
  # plot residuals vs fits
  model <- glm(fn_formula, data = data, family = binomial)
  assp_3 <- plot(model, which = 1)

  # No multicollinearity
  assp_4 <- cor(data[, -1])

  predicted.values <- ifelse(preds >= 0.5, 1, 0)
  check_perf <- data.frame(Y, predicted.values) |> mutate(correct = Y == predicted.values)
  check_perf <- paste0(mean(check_perf$correct) * 100, "% classified correctly")
  return(list(assp_1, assp_2, print(assp_3), assp_4, check_perf))
}

n <- 200
x1 <- rnorm(n, 2, 1)
x2 <- rnorm(n, 4, 1)
prob <- plogis(-1 + 1.2 * x1 - 0.5 * x2)
hist(prob)
y <- rbinom(n, 1, prob)
sim_data <- data.frame(y, x1, x2)

test_logistic_assumptions(fn_formula = "y ~ x1 + x2", data = sim_data)
```

1.  For binary logistic regression, that outcome variables are binary

    Check how many unique outcome variables there are.

2.  Independence of errors

    Check residual plots.

3.  Linear relationship between the outcome variable and log odds of the predictor variables

    Check scatterplots of log odds vs predictor variables to see that there is an approximately linear relationship.

4.  No multicollinearity

    Look at the correlation matrix, generally any value over 0.9 is problematic.

## Breaking assumptions

#### 1. For binary logistic regression, that outcome variables are binary

```{r}
n <- 100
x1 <- rnorm(n, 2, 1)
prob <- plogis(-1 + 0.8 * x1)
y <- rpois(n, prob)
bad_data <- data.frame(y, x1)

test_logistic_assumptions(fn_formula = "y ~ x1", data = bad_data)
```

#### 2. Independence of errors

```{r}
n <- 100
x1 <- rnorm(n, 2, 1)
x2 <- rnorm(n, 0, 1)
prob <- plogis(-1.2 + 0.4 * x1 + 0.3 * x2 + rbeta(n, 2, 2))
hist(prob)
y <- rbinom(n, 1, prob)
bad_data <- data.frame(y, x1, x2)

test_logistic_assumptions(fn_formula = "y ~ x1 + x2", data = bad_data)
```

I used `rbeta` to introduce more error for the middle of the probabilities, which shows in the residual plot, where it looks like there is a peak in the fitted line around 0.

#### 3. Linear relationship between the outcome variable and log odds of the predictor variables

```{r}
n <- 100
x1 <- rnorm(n, 0, 1)
x2 <- rnorm(n, 2, 1)
x3 <- rnorm(n, -2, 1)
prob <- plogis(-1 + x1 * x2 + 1.3 * x2 - 0.5 * x3^2)
hist(prob)
y <- rbinom(n, 1, prob)
bad_data <- data.frame(y, x1, x2, x3)

test_logistic_assumptions(fn_formula = "y ~ x1 + x2 + x3", data = bad_data)
```

For the probability used in the DGP, it is no longer a linear relationship of $X_i\beta$ but one that involves interactions and squared variables. The non-linear relationship is visible in the plots of the outcome variable vs log odds of each predictor variable.

#### 4. No multicollinearity

```{r}
n <- 100
x1 <- rnorm(n, 0, 1)
x2 <- rnorm(n, x1, 0.2)
prob <- plogis(-1 + x1 + 0.5 * x2)
hist(prob)
y <- rbinom(n, 1, prob)
bad_data <- data.frame(y, x1, x2)

test_logistic_assumptions(fn_formula = "y ~ x1 + x2", data = bad_data)
```

The DGP involves $x2$ being created from an `rnorm` around the mean of $x1$, and a small SD so that it is even more correlated. The correlation matrix shows a problematically high correlation. The model still classifies fairly well, but what happens with multicollinearity is that it becomes difficult for the model to estimate the relationship between each predictor variable and the outcome variable independently because the predictor variables tend to change in unison. The p-values are also less trustworthy.

## Next Steps

-   Instead of just using maximum likelihood, we could try using iteratively reweighted least squares or Newton-Ralphson.

## Conclusion

Breaking certain assumptions do not make the logistic model classify worse. Independence of errors seems to be the worst case, but the rest do not change the correct classification rate much.
